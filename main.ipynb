{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_optimizer import Ranger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torchsummary import summary\n",
    "from SE_ResNet_55 import ResNet55, BasicBlock\n",
    "from SE_ResNet_68 import ResNet68, BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/Nikunj-Gupta/Efficient_ResNets/blob/master/lookahead.py\n",
    "\n",
    "'''\n",
    "PyTorch implement of 'Lookahead Optimizer: k steps forward, 1 step back', arXiv:1907.08610\n",
    "'''\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a 55 layers ResNet with 4 residual layers\n",
    "# net = ResNet55(\n",
    "#     block=BasicBlock, \n",
    "#     num_blocks=[2, 2, 2, 2],               # N: number of Residual Layers | Bi:Residual blocks in Residual Layer i \n",
    "#     conv_kernel_sizes=[3, 3, 3, 3],        # Fi: Conv. kernel size in Residual Layer i \n",
    "#     shortcut_kernel_sizes=[1, 1, 1, 1] ,   # Ki: Skip connection kernel size in Residual Layer i \n",
    "#     num_channels=[64, 128, 232, 268],      # Ci: # channels in Residual Layer i \n",
    "#     avg_pool_kernel_size=8,                # P: Average pool kernel size \n",
    "#     drop=0,                                # use dropout with drop proportion \n",
    "#     squeeze_and_excitation=1               # Enable/disable Squeeze-and-Excitation Block \n",
    "#     ) \n",
    "\n",
    "# Define a 68 layers ResNet with 3 residual layers\n",
    "net = ResNet68(\n",
    "    block=BasicBlock, \n",
    "    num_blocks=[4, 4, 3],                    # N: number of Residual Layers | Bi:Residual blocks in Residual Layer i \n",
    "    conv_kernel_sizes=[3, 3, 3],             # Fi: Conv. kernel size in Residual Layer i \n",
    "    shortcut_kernel_sizes=[1, 1, 1] ,        # Ki: Skip connection kernel size in Residual Layer i \n",
    "    num_channels=64,                         # Ci: # channels in Residual Layer i \n",
    "    avg_pool_kernel_size=8,                  # P: Average pool kernel size \n",
    "    drop=0,                                  # use dropout with drop proportion \n",
    "    squeeze_and_excitation=1                 # Enable/disable Squeeze-and-Excitation Block \n",
    "    ) \n",
    "\n",
    "# total_params = 0 \n",
    "# for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "#     total_params += np.prod(x.data.numpy().shape)\n",
    "# print('Total Parameters: ', total_params) \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = net.to(device)\n",
    "\n",
    "# Calculate the number of parameters for training\n",
    "summary(net, input_size=(3, 32, 32))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913996458053589, 0.48215845227241516, 0.44653093814849854), (0.2470322549343109, 0.24348513782024384, 0.26158788800239563))\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913996458053589, 0.48215845227241516, 0.44653093814849854), (0.2470322549343109, 0.24348513782024384, 0.26158788800239563))\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers = 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.1, weight_decay=0.0005)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = Ranger(net.parameters(), lr=0.1, weight_decay=0.0005)\n",
    "optimizer = Lookahead(optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005), k=5, alpha=0.5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_losses = [] \n",
    "    train_acc = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(net.parameters(), clip_value=0.1) \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() \n",
    "\n",
    "        train_acc.append(100.*correct/total) \n",
    "        # print('Batch_idx: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'% (batch_idx, train_loss/(batch_idx+1), 100.*correct/total, correct, total)) \n",
    "    print('train_loss:', np.mean(train_losses)) \n",
    "    print('train_accuracy:', str(np.mean(train_acc)) + '%')\n",
    "    train_loss_per_epoch.append(np.mean(train_losses))\n",
    "    train_acc_per_epoch.append(np.mean(train_acc))\n",
    "    \n",
    "    \n",
    "# Testing \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_losses = [] \n",
    "    test_acc = [] \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() \n",
    "            test_acc.append(100.*correct/total) \n",
    "            # print('Batch_idx: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'% ( batch_idx, test_loss/(batch_idx+1), 100.*correct/total, correct, total)) \n",
    "        print('test_loss:', np.mean(test_losses)) \n",
    "        print('test_accuracy:', str(np.mean(test_acc)) + '%')\n",
    "        test_loss_per_epoch.append(np.mean(test_losses))\n",
    "        test_acc_per_epoch.append(np.mean(test_acc))\n",
    "        \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc: \n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(state, './best.pth')\n",
    "        best_acc = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "train_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "train_acc_per_epoch = []\n",
    "test_acc_per_epoch = []\n",
    "for epoch in range(210): \n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the train and test evaluation metrics per epoch\n",
    "train_test_eval = pd.DataFrame({\n",
    "    \"Train Loss\": train_loss_per_epoch,\n",
    "    \"Train Accuracy\": train_acc_per_epoch,\n",
    "    \"Test Loss\": test_loss_per_epoch,\n",
    "    \"Test Accuracy\": test_acc_per_epoch\n",
    "})\n",
    "train_test_eval.to_csv('./train_test_eval.csv')\n",
    "train_test_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
