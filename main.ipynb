{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we call the modified ResNet architecture (with SEBlock added) defined in SE_ResNet_68.py, and then train and test it with our best set of hyperparameters, training strategies and Ranger optimizer, thus get our best model (with the best test accuracy of 96.48% on CIFAR-10).\n",
    "\n",
    "### In the process of exploring the best model, we experimented on different architectures, hyperparameters and training strategies (the results and plots of experiments are shown in plots_for_augmentation/gradient_clip/lr/optimizer/residual_layers.py). Here we only show the training process and final result of our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_optimizer import Ranger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torchsummary import summary\n",
    "from SE_ResNet_55 import ResNet55, BasicBlock\n",
    "from SE_ResNet_68 import ResNet68, BasicBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Lookahead optimizer for Ranger and further experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/Nikunj-Gupta/Efficient_ResNets/blob/master/lookahead.py\n",
    "\n",
    "'''\n",
    "PyTorch implement of 'Lookahead Optimizer: k steps forward, 1 step back', arXiv:1907.08610\n",
    "'''\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model definiton is in SE_ResNet_55.py and SE_ResNet_68.py (with 4 and 3 residual layers respectively), here we call the architecture and BasicBlock to test both architeture and different sets of hyperparameters. (The plots and results of this experiment are in plots_for_residual_layers.py)\n",
    "\n",
    "### We found the best architure is the one with 3 residual layers, with N = 3, C = 64, Bi = [4, 4, 3], Fi = [3, 3, 3], Ki = [1, 1, 1], P = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      " AdaptiveAvgPool2d-3             [-1, 64, 1, 1]               0\n",
      "            Conv2d-4              [-1, 4, 1, 1]             260\n",
      "              ReLU-5              [-1, 4, 1, 1]               0\n",
      "            Conv2d-6             [-1, 64, 1, 1]             320\n",
      "           Sigmoid-7             [-1, 64, 1, 1]               0\n",
      "           SEBlock-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 32, 32]             128\n",
      "           Conv2d-11           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-13           [-1, 64, 32, 32]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "           Conv2d-16           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-18           [-1, 64, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-23           [-1, 64, 32, 32]               0\n",
      "           Conv2d-24           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
      "           Conv2d-26           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-27           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-28           [-1, 64, 32, 32]               0\n",
      "           Conv2d-29          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-30          [-1, 128, 16, 16]             256\n",
      "           Conv2d-31          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 16, 16]             256\n",
      "           Conv2d-33          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-34          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-35          [-1, 128, 16, 16]               0\n",
      "           Conv2d-36          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-37          [-1, 128, 16, 16]             256\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-40          [-1, 128, 16, 16]               0\n",
      "           Conv2d-41          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
      "           Conv2d-43          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-44          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-45          [-1, 128, 16, 16]               0\n",
      "           Conv2d-46          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 16, 16]             256\n",
      "           Conv2d-48          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-50          [-1, 128, 16, 16]               0\n",
      "           Conv2d-51            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-52            [-1, 256, 8, 8]             512\n",
      "           Conv2d-53            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-54            [-1, 256, 8, 8]             512\n",
      "           Conv2d-55            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-56            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-57            [-1, 256, 8, 8]               0\n",
      "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
      "           Conv2d-60            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-61            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-62            [-1, 256, 8, 8]               0\n",
      "           Conv2d-63            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-64            [-1, 256, 8, 8]             512\n",
      "           Conv2d-65            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-67            [-1, 256, 8, 8]               0\n",
      "           Linear-68                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,697,742\n",
      "Trainable params: 4,697,742\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.13\n",
      "Params size (MB): 17.92\n",
      "Estimated Total Size (MB): 37.06\n",
      "----------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# # Define a 55 layers ResNet with 4 residual layers\n",
    "# net = ResNet55(\n",
    "#     block=BasicBlock, \n",
    "#     num_blocks=[2, 2, 2, 2],               # N: number of Residual Layers | Bi:Residual blocks in Residual Layer i \n",
    "#     conv_kernel_sizes=[3, 3, 3, 3],        # Fi: Conv. kernel size in Residual Layer i \n",
    "#     shortcut_kernel_sizes=[1, 1, 1, 1] ,   # Ki: Skip connection kernel size in Residual Layer i \n",
    "#     num_channels=[64, 128, 232, 268],      # Ci: # channels in Residual Layer i \n",
    "#     avg_pool_kernel_size=8,                # P: Average pool kernel size \n",
    "#     drop=0,                                # use dropout with drop proportion \n",
    "#     squeeze_and_excitation=1               # Enable/disable Squeeze-and-Excitation Block \n",
    "#     ) \n",
    "\n",
    "# Define a 68 layers ResNet with 3 residual layers\n",
    "net = ResNet68(\n",
    "    block=BasicBlock, \n",
    "    num_blocks=[4, 4, 3],                    # N: number of Residual Layers | Bi:Residual blocks in Residual Layer i \n",
    "    conv_kernel_sizes=[3, 3, 3],             # Fi: Conv. kernel size in Residual Layer i \n",
    "    shortcut_kernel_sizes=[1, 1, 1] ,        # Ki: Skip connection kernel size in Residual Layer i \n",
    "    num_channels=64,                         # Ci: # channels in Residual Layer i \n",
    "    avg_pool_kernel_size=8,                  # P: Average pool kernel size \n",
    "    drop=0,                                  # use dropout with drop proportion \n",
    "    squeeze_and_excitation=1                 # Enable/disable Squeeze-and-Excitation Block \n",
    "    ) \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = net.to(device)\n",
    "\n",
    "# Print the architeture and total number of params of the model\n",
    "summary(net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above model summary, we can find that the total number of parameters of our model is 4,697,742 (under the constraint of 5 million)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For data preprocessing, we conducted data augmentation and data normalization on the dataset.\n",
    "### (The plots and results of this experiment are in plots_for_augmentation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913996458053589, 0.48215845227241516, 0.44653093814849854), (0.2470322549343109, 0.24348513782024384, 0.26158788800239563))\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913996458053589, 0.48215845227241516, 0.44653093814849854), (0.2470322549343109, 0.24348513782024384, 0.26158788800239563))\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We tried many different training strategies: For optimizer, we tried SGD, Adam, LookAhead and Ranger. For learning rate, we tried 0.1 and 0.01. (The plots and results of this experiment are in plots_for_optimizer/lr.py)\n",
    "\n",
    "### Through experiments, we found that Ranger optimizer with lr = 0.1 provides the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.1, weight_decay=0.0005)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = Lookahead(optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005), k=5, alpha=0.5)\n",
    "optimizer = Ranger(net.parameters(), lr=0.1, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the training process, we also tried to add Gradient Clipping to stablize it.  (The plots and results of this experiment are in plots_for_gradient_clip.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_losses = [] \n",
    "    train_acc = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(net.parameters(), clip_value=0.1)   # Gradient Clipping: we experimented on whether add this process or not\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() \n",
    "\n",
    "        train_acc.append(100.*correct/total) \n",
    "        # print('Batch_idx: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'% (batch_idx, train_loss/(batch_idx+1), 100.*correct/total, correct, total)) \n",
    "    print('train_loss:', np.mean(train_losses)) \n",
    "    print('train_accuracy:', str(np.mean(train_acc)) + '%')\n",
    "    train_loss_per_epoch.append(np.mean(train_losses))\n",
    "    train_acc_per_epoch.append(np.mean(train_acc))\n",
    "    \n",
    "    \n",
    "# Testing \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_losses = [] \n",
    "    test_acc = [] \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() \n",
    "            test_acc.append(100.*correct/total) \n",
    "            # print('Batch_idx: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'% ( batch_idx, test_loss/(batch_idx+1), 100.*correct/total, correct, total)) \n",
    "        print('test_loss:', np.mean(test_losses)) \n",
    "        print('test_accuracy:', str(np.mean(test_acc)) + '%')\n",
    "        test_loss_per_epoch.append(np.mean(test_losses))\n",
    "        test_acc_per_epoch.append(np.mean(test_acc))\n",
    "        \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc: \n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(state, './best.pth')\n",
    "        best_acc = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "train_loss: 398.5120216037916\n",
      "train_accuracy: 22.477906239512365\n",
      "test_loss: 28.56259649693966\n",
      "test_accuracy: 28.56259649693966\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "train_loss: 307.5882915112064\n",
      "train_accuracy: 41.56876068997233\n",
      "test_loss: 23.551323956251142\n",
      "test_accuracy: 23.551323956251142\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "train_loss: 257.2686211510997\n",
      "train_accuracy: 52.3812932607036\n",
      "test_loss: 23.69196097403765\n",
      "test_accuracy: 23.69196097403765\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "train_loss: 216.0446111706212\n",
      "train_accuracy: 60.48357072980576\n",
      "test_loss: 19.275652007758616\n",
      "test_accuracy: 19.275652007758616\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "train_loss: 185.65784910892893\n",
      "train_accuracy: 66.9213146067268\n",
      "test_loss: 19.59159562140703\n",
      "test_accuracy: 19.59159562140703\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "train_loss: 165.3997737126582\n",
      "train_accuracy: 70.15169789837763\n",
      "test_loss: 13.238767424225808\n",
      "test_accuracy: 13.238767424225808\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "train_loss: 150.87508081612378\n",
      "train_accuracy: 73.36553136812228\n",
      "test_loss: 11.869544850289822\n",
      "test_accuracy: 11.869544850289822\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "train_loss: 141.56304711896135\n",
      "train_accuracy: 74.89507967575031\n",
      "test_loss: 12.82439828068018\n",
      "test_accuracy: 12.82439828068018\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "train_loss: 135.26473526210737\n",
      "train_accuracy: 75.9526908370981\n",
      "test_loss: 11.749099210649728\n",
      "test_accuracy: 11.749099210649728\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "train_loss: 125.01736866574151\n",
      "train_accuracy: 77.92842109547571\n",
      "test_loss: 14.862713167071345\n",
      "test_accuracy: 14.862713167071345\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "train_loss: 121.48488912519896\n",
      "train_accuracy: 78.51866522428038\n",
      "test_loss: 8.840851321071387\n",
      "test_accuracy: 8.840851321071387\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "train_loss: 116.12502665028852\n",
      "train_accuracy: 79.5834227367976\n",
      "test_loss: 10.329842928797008\n",
      "test_accuracy: 10.329842928797008\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "train_loss: 113.94315724650308\n",
      "train_accuracy: 80.08255600023625\n",
      "test_loss: 12.115839321166275\n",
      "test_accuracy: 12.115839321166275\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "train_loss: 109.2270312305454\n",
      "train_accuracy: 80.60947828181467\n",
      "test_loss: 14.263151261210442\n",
      "test_accuracy: 14.263151261210442\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "train_loss: 106.55764245415284\n",
      "train_accuracy: 81.19554855669492\n",
      "test_loss: 11.300986211001874\n",
      "test_accuracy: 11.300986211001874\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "train_loss: 103.56310844954932\n",
      "train_accuracy: 81.49320715657373\n",
      "test_loss: 8.110743064433336\n",
      "test_accuracy: 8.110743064433336\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "train_loss: 102.12454763054848\n",
      "train_accuracy: 82.08789505770848\n",
      "test_loss: 10.466574357450009\n",
      "test_accuracy: 10.466574357450009\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "train_loss: 100.03184251308136\n",
      "train_accuracy: 82.47142278680131\n",
      "test_loss: 9.910887871682643\n",
      "test_accuracy: 9.910887871682643\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "train_loss: 99.1593959421453\n",
      "train_accuracy: 82.41513843394144\n",
      "test_loss: 9.508500860631466\n",
      "test_accuracy: 9.508500860631466\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "train_loss: 95.59374089504752\n",
      "train_accuracy: 83.56092261150128\n",
      "test_loss: 13.03718347325921\n",
      "test_accuracy: 13.03718347325921\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "train_loss: 94.0955282437527\n",
      "train_accuracy: 83.39044073804577\n",
      "test_loss: 7.155796720832586\n",
      "test_accuracy: 7.155796720832586\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "train_loss: 92.26666903617742\n",
      "train_accuracy: 83.83320163058008\n",
      "test_loss: 7.450006913393736\n",
      "test_accuracy: 7.450006913393736\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "train_loss: 91.82985158298936\n",
      "train_accuracy: 83.90799484599314\n",
      "test_loss: 9.225039765238762\n",
      "test_accuracy: 9.225039765238762\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "train_loss: 89.64292919361378\n",
      "train_accuracy: 84.25582111094025\n",
      "test_loss: 9.02720428928733\n",
      "test_accuracy: 9.02720428928733\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "train_loss: 89.03308461549337\n",
      "train_accuracy: 84.33635804941234\n",
      "test_loss: 15.641089165210724\n",
      "test_accuracy: 15.641089165210724\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "train_loss: 88.98136068274603\n",
      "train_accuracy: 84.23318819107365\n",
      "test_loss: 7.081797554716468\n",
      "test_accuracy: 7.081797554716468\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "train_loss: 87.35750619331588\n",
      "train_accuracy: 84.63773867477619\n",
      "test_loss: 7.4461805380880834\n",
      "test_accuracy: 7.4461805380880834\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "train_loss: 86.16771246161302\n",
      "train_accuracy: 84.93719097198101\n",
      "test_loss: 8.08748431429267\n",
      "test_accuracy: 8.08748431429267\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "train_loss: 85.95777269710055\n",
      "train_accuracy: 84.5652025303834\n",
      "test_loss: 10.99820162281394\n",
      "test_accuracy: 10.99820162281394\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "train_loss: 84.79557326382688\n",
      "train_accuracy: 85.13022327143472\n",
      "test_loss: 7.767227613180876\n",
      "test_accuracy: 7.767227613180876\n",
      "\n",
      "\n",
      "Epoch: 30\n",
      "train_loss: 84.12509762692025\n",
      "train_accuracy: 85.10234701477688\n",
      "test_loss: 6.638911524787545\n",
      "test_accuracy: 6.638911524787545\n",
      "\n",
      "\n",
      "Epoch: 31\n",
      "train_loss: 83.67671276426987\n",
      "train_accuracy: 85.64011638612239\n",
      "test_loss: 9.51326494589448\n",
      "test_accuracy: 9.51326494589448\n",
      "\n",
      "\n",
      "Epoch: 32\n",
      "train_loss: 83.22028409558185\n",
      "train_accuracy: 85.30321376186272\n",
      "test_loss: 7.796462482213974\n",
      "test_accuracy: 7.796462482213974\n",
      "\n",
      "\n",
      "Epoch: 33\n",
      "train_loss: 82.56310253885701\n",
      "train_accuracy: 85.7126495218926\n",
      "test_loss: 9.313997247815132\n",
      "test_accuracy: 9.313997247815132\n",
      "\n",
      "\n",
      "Epoch: 34\n",
      "train_loss: 81.86534127413921\n",
      "train_accuracy: 85.48883791594723\n",
      "test_loss: 9.14539493918419\n",
      "test_accuracy: 9.14539493918419\n",
      "\n",
      "\n",
      "Epoch: 35\n",
      "train_loss: 80.37589889993448\n",
      "train_accuracy: 86.19699555697844\n",
      "test_loss: 5.929372611641884\n",
      "test_accuracy: 5.929372611641884\n",
      "\n",
      "\n",
      "Epoch: 36\n",
      "train_loss: 79.71098224067931\n",
      "train_accuracy: 86.24102376159755\n",
      "test_loss: 6.230136569030583\n",
      "test_accuracy: 6.230136569030583\n",
      "\n",
      "\n",
      "Epoch: 37\n",
      "train_loss: 78.34085746212384\n",
      "train_accuracy: 86.44375115422909\n",
      "test_loss: 8.864457020163536\n",
      "test_accuracy: 8.864457020163536\n",
      "\n",
      "\n",
      "Epoch: 38\n",
      "train_loss: 77.99378304610319\n",
      "train_accuracy: 86.36706314680342\n",
      "test_loss: 9.50104426369071\n",
      "test_accuracy: 9.50104426369071\n",
      "\n",
      "\n",
      "Epoch: 39\n",
      "train_loss: 77.81048627506436\n",
      "train_accuracy: 86.69117297311422\n",
      "test_loss: 9.560644910484552\n",
      "test_accuracy: 9.560644910484552\n",
      "\n",
      "\n",
      "Epoch: 40\n",
      "train_loss: 77.60153869640492\n",
      "train_accuracy: 86.50439607717863\n",
      "test_loss: 5.681551395729184\n",
      "test_accuracy: 5.681551395729184\n",
      "\n",
      "\n",
      "Epoch: 41\n",
      "train_loss: 77.87257918585901\n",
      "train_accuracy: 86.3686353199286\n",
      "test_loss: 5.562270036712289\n",
      "test_accuracy: 5.562270036712289\n",
      "\n",
      "\n",
      "Epoch: 42\n",
      "train_loss: 77.81446223852733\n",
      "train_accuracy: 86.21124394604998\n",
      "test_loss: 7.576045615598559\n",
      "test_accuracy: 7.576045615598559\n",
      "\n",
      "\n",
      "Epoch: 43\n",
      "train_loss: 76.20304267565766\n",
      "train_accuracy: 86.8555281198711\n",
      "test_loss: 6.909835837036371\n",
      "test_accuracy: 6.909835837036371\n",
      "\n",
      "\n",
      "Epoch: 44\n",
      "train_loss: 75.51571590177086\n",
      "train_accuracy: 86.99516449162617\n",
      "test_loss: 11.420867233723405\n",
      "test_accuracy: 11.420867233723405\n",
      "\n",
      "\n",
      "Epoch: 45\n",
      "train_loss: 73.71191877606884\n",
      "train_accuracy: 87.31141985840186\n",
      "test_loss: 5.815244195610285\n",
      "test_accuracy: 5.815244195610285\n",
      "\n",
      "\n",
      "Epoch: 46\n",
      "train_loss: 75.48729461827851\n",
      "train_accuracy: 86.47339043216304\n",
      "test_loss: 7.20216554030776\n",
      "test_accuracy: 7.20216554030776\n",
      "\n",
      "\n",
      "Epoch: 47\n",
      "train_loss: 73.65761667642447\n",
      "train_accuracy: 87.05292145469937\n",
      "test_loss: 6.596727951988578\n",
      "test_accuracy: 6.596727951988578\n",
      "\n",
      "\n",
      "Epoch: 48\n",
      "train_loss: 74.71439314902285\n",
      "train_accuracy: 86.75782931110149\n",
      "test_loss: 9.622690009325742\n",
      "test_accuracy: 9.622690009325742\n",
      "\n",
      "\n",
      "Epoch: 49\n",
      "train_loss: 71.87815667422073\n",
      "train_accuracy: 87.3299422987119\n",
      "test_loss: 9.22164303213358\n",
      "test_accuracy: 9.22164303213358\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "train_loss: 73.1372407713281\n",
      "train_accuracy: 87.01175191673413\n",
      "test_loss: 5.544766456261277\n",
      "test_accuracy: 5.544766456261277\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "train_loss: 72.23728202306249\n",
      "train_accuracy: 87.31660958630464\n",
      "test_loss: 5.7754639249294994\n",
      "test_accuracy: 5.7754639249294994\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "train_loss: 72.10136831145914\n",
      "train_accuracy: 87.41594386540237\n",
      "test_loss: 9.05227120667696\n",
      "test_accuracy: 9.05227120667696\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "train_loss: 72.27702638072431\n",
      "train_accuracy: 87.33685008630066\n",
      "test_loss: 6.959715526923537\n",
      "test_accuracy: 6.959715526923537\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "train_loss: 70.88967296885103\n",
      "train_accuracy: 87.72894448361019\n",
      "test_loss: 8.903381114825606\n",
      "test_accuracy: 8.903381114825606\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "train_loss: 69.2373177027687\n",
      "train_accuracy: 88.10839138692461\n",
      "test_loss: 5.3320052023977045\n",
      "test_accuracy: 5.3320052023977045\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "train_loss: 70.09560220564722\n",
      "train_accuracy: 87.58991175268358\n",
      "test_loss: 6.307924540340901\n",
      "test_accuracy: 6.307924540340901\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "train_loss: 69.8058446016153\n",
      "train_accuracy: 87.82264425114425\n",
      "test_loss: 7.367960000038147\n",
      "test_accuracy: 7.367960000038147\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "train_loss: 69.2929907829103\n",
      "train_accuracy: 87.9770918550321\n",
      "test_loss: 6.491409482434392\n",
      "test_accuracy: 6.491409482434392\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "train_loss: 68.51932217714274\n",
      "train_accuracy: 87.83665842232816\n",
      "test_loss: 8.234152866527438\n",
      "test_accuracy: 8.234152866527438\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "train_loss: 68.97008691861501\n",
      "train_accuracy: 88.04497322377145\n",
      "test_loss: 5.35573537684977\n",
      "test_accuracy: 5.35573537684977\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "train_loss: 67.2538876190896\n",
      "train_accuracy: 88.12434502321936\n",
      "test_loss: 5.887897461280227\n",
      "test_accuracy: 5.887897461280227\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "train_loss: 66.92694563763526\n",
      "train_accuracy: 88.33067838789121\n",
      "test_loss: 6.161355244368314\n",
      "test_accuracy: 6.161355244368314\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "train_loss: 67.43673043402717\n",
      "train_accuracy: 88.2173065014983\n",
      "test_loss: 8.319427633285523\n",
      "test_accuracy: 8.319427633285523\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "train_loss: 67.01472915068764\n",
      "train_accuracy: 88.45089511884424\n",
      "test_loss: 7.117523811012506\n",
      "test_accuracy: 7.117523811012506\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "train_loss: 66.92572312781115\n",
      "train_accuracy: 88.14553258789005\n",
      "test_loss: 5.315750595927239\n",
      "test_accuracy: 5.315750595927239\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "train_loss: 66.21053528888604\n",
      "train_accuracy: 88.33780644563483\n",
      "test_loss: 6.515133104845882\n",
      "test_accuracy: 6.515133104845882\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "train_loss: 65.69133166721105\n",
      "train_accuracy: 88.54928394094328\n",
      "test_loss: 6.303458287194371\n",
      "test_accuracy: 6.303458287194371\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "train_loss: 65.64974292411523\n",
      "train_accuracy: 88.50478497374671\n",
      "test_loss: 6.047462334483862\n",
      "test_accuracy: 6.047462334483862\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "train_loss: 65.65627064134763\n",
      "train_accuracy: 88.44598959755245\n",
      "test_loss: 8.168938936293126\n",
      "test_accuracy: 8.168938936293126\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "train_loss: 64.88767143096918\n",
      "train_accuracy: 88.58657220313154\n",
      "test_loss: 5.443719404563308\n",
      "test_accuracy: 5.443719404563308\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "train_loss: 64.1337252902939\n",
      "train_accuracy: 88.65893959524277\n",
      "test_loss: 5.444219310209155\n",
      "test_accuracy: 5.444219310209155\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "train_loss: 64.29798651141736\n",
      "train_accuracy: 88.68142997390363\n",
      "test_loss: 5.814487323909998\n",
      "test_accuracy: 5.814487323909998\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "train_loss: 62.55560331606804\n",
      "train_accuracy: 89.40000648738867\n",
      "test_loss: 7.399399057775736\n",
      "test_accuracy: 7.399399057775736\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "train_loss: 61.56413400702922\n",
      "train_accuracy: 89.43460658472243\n",
      "test_loss: 9.327232185006142\n",
      "test_accuracy: 9.327232185006142\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "train_loss: 61.49799483964968\n",
      "train_accuracy: 89.26796968891723\n",
      "test_loss: 5.453273428976535\n",
      "test_accuracy: 5.453273428976535\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "train_loss: 62.79514959751798\n",
      "train_accuracy: 89.06185907693683\n",
      "test_loss: 5.71952559389174\n",
      "test_accuracy: 5.71952559389174\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "train_loss: 61.99945243148853\n",
      "train_accuracy: 89.24642791990017\n",
      "test_loss: 7.450608198344708\n",
      "test_accuracy: 7.450608198344708\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "train_loss: 60.98131871753184\n",
      "train_accuracy: 89.24991340901543\n",
      "test_loss: 9.81777736544609\n",
      "test_accuracy: 9.81777736544609\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "train_loss: 59.72838327052343\n",
      "train_accuracy: 89.72598109896578\n",
      "test_loss: 6.511450467631221\n",
      "test_accuracy: 6.511450467631221\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "train_loss: 60.327856868962805\n",
      "train_accuracy: 89.1517670446276\n",
      "test_loss: 4.882531569153071\n",
      "test_accuracy: 4.882531569153071\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "train_loss: 59.80807684689684\n",
      "train_accuracy: 89.54009799408432\n",
      "test_loss: 4.801792693883181\n",
      "test_accuracy: 4.801792693883181\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "train_loss: 59.204569962552135\n",
      "train_accuracy: 89.76743413397571\n",
      "test_loss: 5.610434227436781\n",
      "test_accuracy: 5.610434227436781\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "train_loss: 58.28556360148103\n",
      "train_accuracy: 90.08035046377762\n",
      "test_loss: 5.654602019488811\n",
      "test_accuracy: 5.654602019488811\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "train_loss: 59.39372570690749\n",
      "train_accuracy: 89.92761085633533\n",
      "test_loss: 7.41529357554391\n",
      "test_accuracy: 7.41529357554391\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "train_loss: 58.19207633856465\n",
      "train_accuracy: 89.78855613881936\n",
      "test_loss: 4.89859689027071\n",
      "test_accuracy: 4.89859689027071\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "train_loss: 57.52431671847315\n",
      "train_accuracy: 90.10959251019644\n",
      "test_loss: 4.950528017431497\n",
      "test_accuracy: 4.950528017431497\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "train_loss: 57.94654662335468\n",
      "train_accuracy: 90.09729125612397\n",
      "test_loss: 5.8446704622358086\n",
      "test_accuracy: 5.8446704622358086\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "train_loss: 55.937495057692615\n",
      "train_accuracy: 90.46986208920563\n",
      "test_loss: 6.207487746700645\n",
      "test_accuracy: 6.207487746700645\n",
      "\n",
      "\n",
      "Epoch: 89\n",
      "train_loss: 56.04854643653573\n",
      "train_accuracy: 90.07083820990128\n",
      "test_loss: 5.928776534646749\n",
      "test_accuracy: 5.928776534646749\n",
      "\n",
      "\n",
      "Epoch: 90\n",
      "train_loss: 55.33626322272946\n",
      "train_accuracy: 90.44158282958766\n",
      "test_loss: 4.719603335857391\n",
      "test_accuracy: 4.719603335857391\n",
      "\n",
      "\n",
      "Epoch: 91\n",
      "train_loss: 55.2496053827998\n",
      "train_accuracy: 90.24149993003653\n",
      "test_loss: 5.834301806241274\n",
      "test_accuracy: 5.834301806241274\n",
      "\n",
      "\n",
      "Epoch: 92\n",
      "train_loss: 54.65192419292448\n",
      "train_accuracy: 90.57984305646052\n",
      "test_loss: 5.68150135204196\n",
      "test_accuracy: 5.68150135204196\n",
      "\n",
      "\n",
      "Epoch: 93\n",
      "train_loss: 54.72512809493963\n",
      "train_accuracy: 90.4452298967576\n",
      "test_loss: 5.777464063093066\n",
      "test_accuracy: 5.777464063093066\n",
      "\n",
      "\n",
      "Epoch: 94\n",
      "train_loss: 52.79245008493933\n",
      "train_accuracy: 90.71192723808184\n",
      "test_loss: 6.667330345511436\n",
      "test_accuracy: 6.667330345511436\n",
      "\n",
      "\n",
      "Epoch: 95\n",
      "train_loss: 55.12626379116646\n",
      "train_accuracy: 90.30071433713869\n",
      "test_loss: 4.632089300453663\n",
      "test_accuracy: 4.632089300453663\n",
      "\n",
      "\n",
      "Epoch: 96\n",
      "train_loss: 53.90957546026429\n",
      "train_accuracy: 90.53791339779464\n",
      "test_loss: 4.934445177763701\n",
      "test_accuracy: 4.934445177763701\n",
      "\n",
      "\n",
      "Epoch: 97\n",
      "train_loss: 53.55371780461058\n",
      "train_accuracy: 90.63975406586395\n",
      "test_loss: 6.803760772198439\n",
      "test_accuracy: 6.803760772198439\n",
      "\n",
      "\n",
      "Epoch: 98\n",
      "train_loss: 53.065968553428455\n",
      "train_accuracy: 90.67613010993028\n",
      "test_loss: 6.275424585863948\n",
      "test_accuracy: 6.275424585863948\n",
      "\n",
      "\n",
      "Epoch: 99\n",
      "train_loss: 51.94471315311654\n",
      "train_accuracy: 91.0726564185314\n",
      "test_loss: 7.437609111517668\n",
      "test_accuracy: 7.437609111517668\n",
      "\n",
      "\n",
      "Epoch: 100\n",
      "train_loss: 50.45435645943865\n",
      "train_accuracy: 91.06629363888804\n",
      "test_loss: 4.445081152766943\n",
      "test_accuracy: 4.445081152766943\n",
      "\n",
      "\n",
      "Epoch: 101\n",
      "train_loss: 50.33451856055376\n",
      "train_accuracy: 91.1899365198438\n",
      "test_loss: 4.790747775137424\n",
      "test_accuracy: 4.790747775137424\n",
      "\n",
      "\n",
      "Epoch: 102\n",
      "train_loss: 50.58502208691119\n",
      "train_accuracy: 91.38886864141134\n",
      "test_loss: 5.3943371590226885\n",
      "test_accuracy: 5.3943371590226885\n",
      "\n",
      "\n",
      "Epoch: 103\n",
      "train_loss: 49.72478514970721\n",
      "train_accuracy: 91.36620710427594\n",
      "test_loss: 5.20222186781466\n",
      "test_accuracy: 5.20222186781466\n",
      "\n",
      "\n",
      "Epoch: 104\n",
      "train_loss: 49.325955990825776\n",
      "train_accuracy: 91.2475227449718\n",
      "test_loss: 5.088807224854827\n",
      "test_accuracy: 5.088807224854827\n",
      "\n",
      "\n",
      "Epoch: 105\n",
      "train_loss: 49.56471201316322\n",
      "train_accuracy: 91.38009667015392\n",
      "test_loss: 4.637947217002511\n",
      "test_accuracy: 4.637947217002511\n",
      "\n",
      "\n",
      "Epoch: 106\n",
      "train_loss: 48.88124049018563\n",
      "train_accuracy: 91.50734115624194\n",
      "test_loss: 4.539990048855543\n",
      "test_accuracy: 4.539990048855543\n",
      "\n",
      "\n",
      "Epoch: 107\n",
      "train_loss: 47.47593290090104\n",
      "train_accuracy: 91.69660058965714\n",
      "test_loss: 5.3867970366030935\n",
      "test_accuracy: 5.3867970366030935\n",
      "\n",
      "\n",
      "Epoch: 108\n",
      "train_loss: 48.12119970884165\n",
      "train_accuracy: 91.3761206377602\n",
      "test_loss: 4.957128394022584\n",
      "test_accuracy: 4.957128394022584\n",
      "\n",
      "\n",
      "Epoch: 109\n",
      "train_loss: 47.30312422222799\n",
      "train_accuracy: 91.89422924355742\n",
      "test_loss: 7.810837483778596\n",
      "test_accuracy: 7.810837483778596\n",
      "\n",
      "\n",
      "Epoch: 110\n",
      "train_loss: 46.102939844302966\n",
      "train_accuracy: 92.0299553525571\n",
      "test_loss: 4.456182098016143\n",
      "test_accuracy: 4.456182098016143\n",
      "\n",
      "\n",
      "Epoch: 111\n",
      "train_loss: 46.8726566266602\n",
      "train_accuracy: 91.97965722168\n",
      "test_loss: 4.714265663176775\n",
      "test_accuracy: 4.714265663176775\n",
      "\n",
      "\n",
      "Epoch: 112\n",
      "train_loss: 45.85861268948258\n",
      "train_accuracy: 91.9268550835085\n",
      "test_loss: 4.756800250336528\n",
      "test_accuracy: 4.756800250336528\n",
      "\n",
      "\n",
      "Epoch: 113\n",
      "train_loss: 44.86006511740215\n",
      "train_accuracy: 92.27846525779628\n",
      "test_loss: 4.844113847613334\n",
      "test_accuracy: 4.844113847613334\n",
      "\n",
      "\n",
      "Epoch: 114\n",
      "train_loss: 44.75391006818437\n",
      "train_accuracy: 92.15739276997732\n",
      "test_loss: 4.664315938204527\n",
      "test_accuracy: 4.664315938204527\n",
      "\n",
      "\n",
      "Epoch: 115\n",
      "train_loss: 44.11653649120989\n",
      "train_accuracy: 92.23501644114744\n",
      "test_loss: 4.059504763036967\n",
      "test_accuracy: 4.059504763036967\n",
      "\n",
      "\n",
      "Epoch: 116\n",
      "train_loss: 42.30398310900039\n",
      "train_accuracy: 92.67916083742284\n",
      "test_loss: 4.246927292644978\n",
      "test_accuracy: 4.246927292644978\n",
      "\n",
      "\n",
      "Epoch: 117\n",
      "train_loss: 43.043999174290605\n",
      "train_accuracy: 92.6133958756232\n",
      "test_loss: 4.749554412066937\n",
      "test_accuracy: 4.749554412066937\n",
      "\n",
      "\n",
      "Epoch: 118\n",
      "train_loss: 42.88779198959508\n",
      "train_accuracy: 92.37271238060713\n",
      "test_loss: 5.72185538224876\n",
      "test_accuracy: 5.72185538224876\n",
      "\n",
      "\n",
      "Epoch: 119\n",
      "train_loss: 40.783420475947736\n",
      "train_accuracy: 93.05656551098058\n",
      "test_loss: 5.008225696906448\n",
      "test_accuracy: 5.008225696906448\n",
      "\n",
      "\n",
      "Epoch: 120\n",
      "train_loss: 41.52972507210034\n",
      "train_accuracy: 92.93972067512378\n",
      "test_loss: 3.862403558380902\n",
      "test_accuracy: 3.862403558380902\n",
      "\n",
      "\n",
      "Epoch: 121\n",
      "train_loss: 42.30231691595844\n",
      "train_accuracy: 92.747818509132\n",
      "test_loss: 4.051818858459592\n",
      "test_accuracy: 4.051818858459592\n",
      "\n",
      "\n",
      "Epoch: 122\n",
      "train_loss: 40.28960144902816\n",
      "train_accuracy: 92.96493763885022\n",
      "test_loss: 5.225495221838355\n",
      "test_accuracy: 5.225495221838355\n",
      "\n",
      "\n",
      "Epoch: 123\n",
      "train_loss: 40.94959391941271\n",
      "train_accuracy: 92.88554769244844\n",
      "test_loss: 4.810104943439365\n",
      "test_accuracy: 4.810104943439365\n",
      "\n",
      "\n",
      "Epoch: 124\n",
      "train_loss: 39.26155614711897\n",
      "train_accuracy: 93.31640973861224\n",
      "test_loss: 6.481406252458692\n",
      "test_accuracy: 6.481406252458692\n",
      "\n",
      "\n",
      "Epoch: 125\n",
      "train_loss: 39.82061778517711\n",
      "train_accuracy: 92.96755502204952\n",
      "test_loss: 4.052829557657242\n",
      "test_accuracy: 4.052829557657242\n",
      "\n",
      "\n",
      "Epoch: 126\n",
      "train_loss: 38.551477600794165\n",
      "train_accuracy: 93.25862483513788\n",
      "test_loss: 4.233464972674847\n",
      "test_accuracy: 4.233464972674847\n",
      "\n",
      "\n",
      "Epoch: 127\n",
      "train_loss: 38.138358163349615\n",
      "train_accuracy: 93.5174095181839\n",
      "test_loss: 3.88656633682549\n",
      "test_accuracy: 3.88656633682549\n",
      "\n",
      "\n",
      "Epoch: 128\n",
      "train_loss: 37.8245650422009\n",
      "train_accuracy: 93.37516603141384\n",
      "test_loss: 5.12716023735702\n",
      "test_accuracy: 5.12716023735702\n",
      "\n",
      "\n",
      "Epoch: 129\n",
      "train_loss: 37.63458349515715\n",
      "train_accuracy: 93.58597103836476\n",
      "test_loss: 4.815935921296477\n",
      "test_accuracy: 4.815935921296477\n",
      "\n",
      "\n",
      "Epoch: 130\n",
      "train_loss: 36.525277591517664\n",
      "train_accuracy: 93.54796706977216\n",
      "test_loss: 3.886155324801803\n",
      "test_accuracy: 3.886155324801803\n",
      "\n",
      "\n",
      "Epoch: 131\n",
      "train_loss: 35.552377292014604\n",
      "train_accuracy: 93.7809197434704\n",
      "test_loss: 4.47772114276886\n",
      "test_accuracy: 4.47772114276886\n",
      "\n",
      "\n",
      "Epoch: 132\n",
      "train_loss: 35.60355892365851\n",
      "train_accuracy: 93.99846206698916\n",
      "test_loss: 4.844700932502747\n",
      "test_accuracy: 4.844700932502747\n",
      "\n",
      "\n",
      "Epoch: 133\n",
      "train_loss: 34.536378020768424\n",
      "train_accuracy: 94.06172390511516\n",
      "test_loss: 5.319546214863658\n",
      "test_accuracy: 5.319546214863658\n",
      "\n",
      "\n",
      "Epoch: 134\n",
      "train_loss: 34.084949314841985\n",
      "train_accuracy: 94.00751617169928\n",
      "test_loss: 4.731380984559655\n",
      "test_accuracy: 4.731380984559655\n",
      "\n",
      "\n",
      "Epoch: 135\n",
      "train_loss: 33.930344803711336\n",
      "train_accuracy: 94.16899902216596\n",
      "test_loss: 3.644306928478181\n",
      "test_accuracy: 3.644306928478181\n",
      "\n",
      "\n",
      "Epoch: 136\n",
      "train_loss: 33.53041386998751\n",
      "train_accuracy: 94.2982726064502\n",
      "test_loss: 4.083662728592754\n",
      "test_accuracy: 4.083662728592754\n",
      "\n",
      "\n",
      "Epoch: 137\n",
      "train_loss: 33.126297470649035\n",
      "train_accuracy: 94.37186689271888\n",
      "test_loss: 3.892431452311576\n",
      "test_accuracy: 3.892431452311576\n",
      "\n",
      "\n",
      "Epoch: 138\n",
      "train_loss: 32.42821350545072\n",
      "train_accuracy: 94.26746245698362\n",
      "test_loss: 4.422094421461225\n",
      "test_accuracy: 4.422094421461225\n",
      "\n",
      "\n",
      "Epoch: 139\n",
      "train_loss: 32.40673084437009\n",
      "train_accuracy: 94.42763702042164\n",
      "test_loss: 4.25771736651659\n",
      "test_accuracy: 4.25771736651659\n",
      "\n",
      "\n",
      "Epoch: 140\n",
      "train_loss: 31.660995025464032\n",
      "train_accuracy: 94.29525978392334\n",
      "test_loss: 3.80276553183794\n",
      "test_accuracy: 3.80276553183794\n",
      "\n",
      "\n",
      "Epoch: 141\n",
      "train_loss: 31.28660548434538\n",
      "train_accuracy: 94.48118397379864\n",
      "test_loss: 3.5274814205244183\n",
      "test_accuracy: 3.5274814205244183\n",
      "\n",
      "\n",
      "Epoch: 142\n",
      "train_loss: 28.772355690884314\n",
      "train_accuracy: 95.23338933935348\n",
      "test_loss: 3.735160085558891\n",
      "test_accuracy: 3.735160085558891\n",
      "\n",
      "\n",
      "Epoch: 143\n",
      "train_loss: 30.0643166657966\n",
      "train_accuracy: 94.78472234716196\n",
      "test_loss: 3.8223219752311706\n",
      "test_accuracy: 3.8223219752311706\n",
      "\n",
      "\n",
      "Epoch: 144\n",
      "train_loss: 29.69952713482825\n",
      "train_accuracy: 94.86269003600246\n",
      "test_loss: 4.123067647218704\n",
      "test_accuracy: 4.123067647218704\n",
      "\n",
      "\n",
      "Epoch: 145\n",
      "train_loss: 28.088275553158407\n",
      "train_accuracy: 95.16004801141196\n",
      "test_loss: 3.620321821793914\n",
      "test_accuracy: 3.620321821793914\n",
      "\n",
      "\n",
      "Epoch: 146\n",
      "train_loss: 26.921882062721664\n",
      "train_accuracy: 95.44170519393106\n",
      "test_loss: 3.55070296023041\n",
      "test_accuracy: 3.55070296023041\n",
      "\n",
      "\n",
      "Epoch: 147\n",
      "train_loss: 26.97256911022927\n",
      "train_accuracy: 95.38375829270726\n",
      "test_loss: 3.3289082046598195\n",
      "test_accuracy: 3.3289082046598195\n",
      "\n",
      "\n",
      "Epoch: 148\n",
      "train_loss: 26.272064578131108\n",
      "train_accuracy: 95.4544886273999\n",
      "test_loss: 3.719563727080822\n",
      "test_accuracy: 3.719563727080822\n",
      "\n",
      "\n",
      "Epoch: 149\n",
      "train_loss: 26.28430150957097\n",
      "train_accuracy: 95.35199115750116\n",
      "test_loss: 3.6396853532642126\n",
      "test_accuracy: 3.6396853532642126\n",
      "\n",
      "\n",
      "Epoch: 150\n",
      "train_loss: 24.52477890066326\n",
      "train_accuracy: 95.845157409522\n",
      "test_loss: 3.310100410319865\n",
      "test_accuracy: 3.310100410319865\n",
      "\n",
      "\n",
      "Epoch: 151\n",
      "train_loss: 25.46973627114959\n",
      "train_accuracy: 95.56602348766592\n",
      "test_loss: 3.413228993304074\n",
      "test_accuracy: 3.413228993304074\n",
      "\n",
      "\n",
      "Epoch: 152\n",
      "train_loss: 24.581395103327\n",
      "train_accuracy: 95.77786629119872\n",
      "test_loss: 3.417898713052273\n",
      "test_accuracy: 3.417898713052273\n",
      "\n",
      "\n",
      "Epoch: 153\n",
      "train_loss: 23.109727467836628\n",
      "train_accuracy: 96.15924958575428\n",
      "test_loss: 3.355408019106835\n",
      "test_accuracy: 3.355408019106835\n",
      "\n",
      "\n",
      "Epoch: 154\n",
      "train_loss: 23.7317045685809\n",
      "train_accuracy: 95.96643409673015\n",
      "test_loss: 3.60483514033258\n",
      "test_accuracy: 3.60483514033258\n",
      "\n",
      "\n",
      "Epoch: 155\n",
      "train_loss: 22.89336297522916\n",
      "train_accuracy: 96.04292642395195\n",
      "test_loss: 3.4141456678509714\n",
      "test_accuracy: 3.4141456678509714\n",
      "\n",
      "\n",
      "Epoch: 156\n",
      "train_loss: 21.998097156396\n",
      "train_accuracy: 96.20477387118034\n",
      "test_loss: 3.40007101893425\n",
      "test_accuracy: 3.40007101893425\n",
      "\n",
      "\n",
      "Epoch: 157\n",
      "train_loss: 20.844130564714447\n",
      "train_accuracy: 96.37419725204772\n",
      "test_loss: 3.2411256806924937\n",
      "test_accuracy: 3.2411256806924937\n",
      "\n",
      "\n",
      "Epoch: 158\n",
      "train_loss: 21.81447214336918\n",
      "train_accuracy: 96.19099325928704\n",
      "test_loss: 3.3116093600168823\n",
      "test_accuracy: 3.3116093600168823\n",
      "\n",
      "\n",
      "Epoch: 159\n",
      "train_loss: 20.33661927311393\n",
      "train_accuracy: 96.60235596840364\n",
      "test_loss: 3.420148734748364\n",
      "test_accuracy: 3.420148734748364\n",
      "\n",
      "\n",
      "Epoch: 160\n",
      "train_loss: 20.871146228707502\n",
      "train_accuracy: 96.4391629367731\n",
      "test_loss: 3.3444987002760174\n",
      "test_accuracy: 3.3444987002760174\n",
      "\n",
      "\n",
      "Epoch: 161\n",
      "train_loss: 20.22493544851651\n",
      "train_accuracy: 96.59325281959012\n",
      "test_loss: 3.518172740377486\n",
      "test_accuracy: 3.518172740377486\n",
      "\n",
      "\n",
      "Epoch: 162\n",
      "train_loss: 18.668906950036927\n",
      "train_accuracy: 96.71949277851974\n",
      "test_loss: 3.466076799668372\n",
      "test_accuracy: 3.466076799668372\n",
      "\n",
      "\n",
      "Epoch: 163\n",
      "train_loss: 19.79005107220234\n",
      "train_accuracy: 96.64921613143034\n",
      "test_loss: 3.2718501070514323\n",
      "test_accuracy: 3.2718501070514323\n",
      "\n",
      "\n",
      "Epoch: 164\n",
      "train_loss: 18.838994469312603\n",
      "train_accuracy: 96.70470471227289\n",
      "test_loss: 3.3992221327498555\n",
      "test_accuracy: 3.3992221327498555\n",
      "\n",
      "\n",
      "Epoch: 165\n",
      "train_loss: 17.863097721829895\n",
      "train_accuracy: 97.05568612209308\n",
      "test_loss: 3.075133962929249\n",
      "test_accuracy: 3.075133962929249\n",
      "\n",
      "\n",
      "Epoch: 166\n",
      "train_loss: 17.395480157050024\n",
      "train_accuracy: 97.11960447060916\n",
      "test_loss: 2.97221896648407\n",
      "test_accuracy: 2.97221896648407\n",
      "\n",
      "\n",
      "Epoch: 167\n",
      "train_loss: 18.13369663165944\n",
      "train_accuracy: 96.86499386667592\n",
      "test_loss: 3.1069085724651813\n",
      "test_accuracy: 3.1069085724651813\n",
      "\n",
      "\n",
      "Epoch: 168\n",
      "train_loss: 17.52090673453039\n",
      "train_accuracy: 97.0562088978148\n",
      "test_loss: 3.262975288182497\n",
      "test_accuracy: 3.262975288182497\n",
      "\n",
      "\n",
      "Epoch: 169\n",
      "train_loss: 16.638846804978577\n",
      "train_accuracy: 97.14433317377384\n",
      "test_loss: 3.031508517079055\n",
      "test_accuracy: 3.031508517079055\n",
      "\n",
      "\n",
      "Epoch: 170\n",
      "train_loss: 16.219117463130477\n",
      "train_accuracy: 97.33137942239256\n",
      "test_loss: 2.8489478031173348\n",
      "test_accuracy: 2.8489478031173348\n",
      "\n",
      "\n",
      "Epoch: 171\n",
      "train_loss: 16.16225239852696\n",
      "train_accuracy: 97.27272758708182\n",
      "test_loss: 2.8146222062408923\n",
      "test_accuracy: 2.8146222062408923\n",
      "\n",
      "\n",
      "Epoch: 172\n",
      "train_loss: 15.885603806080148\n",
      "train_accuracy: 97.26542874202867\n",
      "test_loss: 2.91990106087178\n",
      "test_accuracy: 2.91990106087178\n",
      "\n",
      "\n",
      "Epoch: 173\n",
      "train_loss: 15.50686535677251\n",
      "train_accuracy: 97.4035807436262\n",
      "test_loss: 3.039909387752414\n",
      "test_accuracy: 3.039909387752414\n",
      "\n",
      "\n",
      "Epoch: 174\n",
      "train_loss: 14.713160821620155\n",
      "train_accuracy: 97.61696499766283\n",
      "test_loss: 2.921680192556232\n",
      "test_accuracy: 2.921680192556232\n",
      "\n",
      "\n",
      "Epoch: 175\n",
      "train_loss: 14.755781359751436\n",
      "train_accuracy: 97.50958742537726\n",
      "test_loss: 2.926136116683483\n",
      "test_accuracy: 2.926136116683483\n",
      "\n",
      "\n",
      "Epoch: 176\n",
      "train_loss: 14.184960509852871\n",
      "train_accuracy: 97.69050874233922\n",
      "test_loss: 2.698594422452152\n",
      "test_accuracy: 2.698594422452152\n",
      "\n",
      "\n",
      "Epoch: 177\n",
      "train_loss: 13.564254276935593\n",
      "train_accuracy: 97.73826655552394\n",
      "test_loss: 2.709358849562705\n",
      "test_accuracy: 2.709358849562705\n",
      "\n",
      "\n",
      "Epoch: 178\n",
      "train_loss: 14.146712982393517\n",
      "train_accuracy: 97.52410360702828\n",
      "test_loss: 2.761180497519672\n",
      "test_accuracy: 2.761180497519672\n",
      "\n",
      "\n",
      "Epoch: 179\n",
      "train_loss: 14.08221529402277\n",
      "train_accuracy: 97.51122916525328\n",
      "test_loss: 2.780547771602869\n",
      "test_accuracy: 2.780547771602869\n",
      "\n",
      "\n",
      "Epoch: 180\n",
      "train_loss: 13.61980902293192\n",
      "train_accuracy: 97.70694226964858\n",
      "test_loss: 2.729340844880789\n",
      "test_accuracy: 2.729340844880789\n",
      "\n",
      "\n",
      "Epoch: 181\n",
      "train_loss: 13.129225427997024\n",
      "train_accuracy: 97.79621108381863\n",
      "test_loss: 2.6771572642959653\n",
      "test_accuracy: 2.6771572642959653\n",
      "\n",
      "\n",
      "Epoch: 182\n",
      "train_loss: 13.29284818247056\n",
      "train_accuracy: 97.80585763430018\n",
      "test_loss: 2.6870766416192056\n",
      "test_accuracy: 2.6870766416192056\n",
      "\n",
      "\n",
      "Epoch: 183\n",
      "train_loss: 13.002804440915432\n",
      "train_accuracy: 97.8120578597829\n",
      "test_loss: 2.7183305392041803\n",
      "test_accuracy: 2.7183305392041803\n",
      "\n",
      "\n",
      "Epoch: 184\n",
      "train_loss: 13.13550042454868\n",
      "train_accuracy: 97.71703161570689\n",
      "test_loss: 2.696632570028305\n",
      "test_accuracy: 2.696632570028305\n",
      "\n",
      "\n",
      "Epoch: 185\n",
      "train_loss: 12.521920219249546\n",
      "train_accuracy: 97.86964972627588\n",
      "test_loss: 2.65219805855304\n",
      "test_accuracy: 2.65219805855304\n",
      "\n",
      "\n",
      "Epoch: 186\n",
      "train_loss: 13.348459566587849\n",
      "train_accuracy: 97.72601043078758\n",
      "test_loss: 2.62554414588958\n",
      "test_accuracy: 2.62554414588958\n",
      "\n",
      "\n",
      "Epoch: 187\n",
      "train_loss: 11.94344663095858\n",
      "train_accuracy: 98.02918036598506\n",
      "test_loss: 2.5952853355556726\n",
      "test_accuracy: 2.5952853355556726\n",
      "\n",
      "\n",
      "Epoch: 188\n",
      "train_loss: 12.084947726963197\n",
      "train_accuracy: 98.01420564836523\n",
      "test_loss: 2.587182148452848\n",
      "test_accuracy: 2.587182148452848\n",
      "\n",
      "\n",
      "Epoch: 189\n",
      "train_loss: 11.99771870272782\n",
      "train_accuracy: 97.96180271663891\n",
      "test_loss: 2.600783080421388\n",
      "test_accuracy: 2.600783080421388\n",
      "\n",
      "\n",
      "Epoch: 190\n",
      "train_loss: 12.442503222085708\n",
      "train_accuracy: 97.78720314113096\n",
      "test_loss: 2.5176296254619954\n",
      "test_accuracy: 2.5176296254619954\n",
      "\n",
      "\n",
      "Epoch: 191\n",
      "train_loss: 11.785633399613356\n",
      "train_accuracy: 98.02204353648015\n",
      "test_loss: 2.6025912066921597\n",
      "test_accuracy: 2.6025912066921597\n",
      "\n",
      "\n",
      "Epoch: 192\n",
      "train_loss: 11.963230087636203\n",
      "train_accuracy: 97.96296259445664\n",
      "test_loss: 2.571906881593168\n",
      "test_accuracy: 2.571906881593168\n",
      "\n",
      "\n",
      "Epoch: 193\n",
      "train_loss: 11.893491982873003\n",
      "train_accuracy: 97.9915050984634\n",
      "test_loss: 2.551341654546559\n",
      "test_accuracy: 2.551341654546559\n",
      "\n",
      "\n",
      "Epoch: 194\n",
      "train_loss: 11.93380372146921\n",
      "train_accuracy: 97.9735645682984\n",
      "test_loss: 2.5523005282506346\n",
      "test_accuracy: 2.5523005282506346\n",
      "\n",
      "\n",
      "Epoch: 195\n",
      "train_loss: 11.774385955470526\n",
      "train_accuracy: 98.035816570464\n",
      "test_loss: 2.512700429558754\n",
      "test_accuracy: 2.512700429558754\n",
      "\n",
      "\n",
      "Epoch: 196\n",
      "train_loss: 11.75416640516566\n",
      "train_accuracy: 97.91440307180387\n",
      "test_loss: 2.5592490628361704\n",
      "test_accuracy: 2.5592490628361704\n",
      "\n",
      "\n",
      "Epoch: 197\n",
      "train_loss: 11.526641575450464\n",
      "train_accuracy: 98.05831533383864\n",
      "test_loss: 2.5320591520518065\n",
      "test_accuracy: 2.5320591520518065\n",
      "\n",
      "\n",
      "Epoch: 198\n",
      "train_loss: 12.057734209406988\n",
      "train_accuracy: 97.86738624442484\n",
      "test_loss: 2.5406643634662034\n",
      "test_accuracy: 2.5406643634662034\n",
      "\n",
      "\n",
      "Epoch: 199\n",
      "train_loss: 12.315271101744315\n",
      "train_accuracy: 97.96750138778444\n",
      "test_loss: 2.592754622921348\n",
      "test_accuracy: 2.592754622921348\n",
      "\n",
      "\n",
      "Epoch: 200\n",
      "train_loss: 11.383592943186915\n",
      "train_accuracy: 98.14209414842648\n",
      "test_loss: 2.534286229126155\n",
      "test_accuracy: 2.534286229126155\n",
      "\n",
      "\n",
      "Epoch: 201\n",
      "train_loss: 11.968271058201648\n",
      "train_accuracy: 98.01611447685748\n",
      "test_loss: 2.5562696872279047\n",
      "test_accuracy: 2.5562696872279047\n",
      "\n",
      "\n",
      "Epoch: 202\n",
      "train_loss: 11.78455930901334\n",
      "train_accuracy: 97.95804578746018\n",
      "test_loss: 2.529613794386387\n",
      "test_accuracy: 2.529613794386387\n",
      "\n",
      "\n",
      "Epoch: 203\n",
      "train_loss: 11.400317215982376\n",
      "train_accuracy: 98.09046640408847\n",
      "test_loss: 2.5465671569108963\n",
      "test_accuracy: 2.5465671569108963\n",
      "\n",
      "\n",
      "Epoch: 204\n",
      "train_loss: 12.037255931952895\n",
      "train_accuracy: 97.97465277569516\n",
      "test_loss: 2.5300069795921445\n",
      "test_accuracy: 2.5300069795921445\n",
      "\n",
      "\n",
      "Epoch: 205\n",
      "train_loss: 11.540171387151616\n",
      "train_accuracy: 98.09174275273892\n",
      "test_loss: 2.5391257479786877\n",
      "test_accuracy: 2.5391257479786877\n",
      "\n",
      "\n",
      "Epoch: 206\n",
      "train_loss: 11.500224660309105\n",
      "train_accuracy: 98.09122758305456\n",
      "test_loss: 2.525335049815476\n",
      "test_accuracy: 2.525335049815476\n",
      "\n",
      "\n",
      "Epoch: 207\n",
      "train_loss: 11.40854895962974\n",
      "train_accuracy: 98.03232515314436\n",
      "test_loss: 2.521021692827344\n",
      "test_accuracy: 2.521021692827344\n",
      "\n",
      "\n",
      "Epoch: 208\n",
      "train_loss: 11.52365294746016\n",
      "train_accuracy: 98.0598809727994\n",
      "test_loss: 2.5145475205034016\n",
      "test_accuracy: 2.5145475205034016\n",
      "\n",
      "\n",
      "Epoch: 209\n",
      "train_loss: 11.85055590337833\n",
      "train_accuracy: 97.93830538120828\n",
      "test_loss: 2.5678089510649444\n",
      "test_accuracy: 2.5678089510649444\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "train_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "train_acc_per_epoch = []\n",
    "test_acc_per_epoch = []\n",
    "for epoch in range(210): \n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the train and test evaluation metrics per epoch\n",
    "train_test_eval = pd.DataFrame({\n",
    "    \"Train Loss\": train_loss_per_epoch,\n",
    "    \"Train Accuracy\": train_acc_per_epoch,\n",
    "    \"Test Loss\": test_loss_per_epoch,\n",
    "    \"Test Accuracy\": test_acc_per_epoch\n",
    "})\n",
    "train_test_eval.to_csv('./train_test_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1619.177015</td>\n",
       "      <td>20.226219</td>\n",
       "      <td>132.875744</td>\n",
       "      <td>40.173329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243.904046</td>\n",
       "      <td>41.361891</td>\n",
       "      <td>87.739549</td>\n",
       "      <td>59.356718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1008.477470</td>\n",
       "      <td>53.931819</td>\n",
       "      <td>112.362868</td>\n",
       "      <td>55.608871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>863.560530</td>\n",
       "      <td>60.753004</td>\n",
       "      <td>64.298454</td>\n",
       "      <td>71.703157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>780.544254</td>\n",
       "      <td>64.604683</td>\n",
       "      <td>140.398815</td>\n",
       "      <td>53.969890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>64.926888</td>\n",
       "      <td>97.199416</td>\n",
       "      <td>9.828154</td>\n",
       "      <td>95.998920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>63.866885</td>\n",
       "      <td>97.318172</td>\n",
       "      <td>9.817834</td>\n",
       "      <td>95.925415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>66.104326</td>\n",
       "      <td>97.154664</td>\n",
       "      <td>9.927568</td>\n",
       "      <td>95.725125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>65.536809</td>\n",
       "      <td>97.259325</td>\n",
       "      <td>9.769878</td>\n",
       "      <td>95.851526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>65.436616</td>\n",
       "      <td>97.396771</td>\n",
       "      <td>9.687873</td>\n",
       "      <td>96.071812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train Loss  Train Accuracy   Test Loss  Test Accuracy\n",
       "0    1619.177015       20.226219  132.875744      40.173329\n",
       "1    1243.904046       41.361891   87.739549      59.356718\n",
       "2    1008.477470       53.931819  112.362868      55.608871\n",
       "3     863.560530       60.753004   64.298454      71.703157\n",
       "4     780.544254       64.604683  140.398815      53.969890\n",
       "..           ...             ...         ...            ...\n",
       "205    64.926888       97.199416    9.828154      95.998920\n",
       "206    63.866885       97.318172    9.817834      95.925415\n",
       "207    66.104326       97.154664    9.927568      95.725125\n",
       "208    65.536809       97.259325    9.769878      95.851526\n",
       "209    65.436616       97.396771    9.687873      96.071812\n",
       "\n",
       "[210 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the final test accuracy of our modified ResNet is 96.48% after trained for 209 epochs (The model was saved after this epoch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398.512022</td>\n",
       "      <td>22.477906</td>\n",
       "      <td>28.562596</td>\n",
       "      <td>49.349745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307.588292</td>\n",
       "      <td>41.568761</td>\n",
       "      <td>23.551324</td>\n",
       "      <td>59.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257.268621</td>\n",
       "      <td>52.381293</td>\n",
       "      <td>23.691961</td>\n",
       "      <td>61.859415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216.044611</td>\n",
       "      <td>60.483571</td>\n",
       "      <td>19.275652</td>\n",
       "      <td>68.197768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185.657849</td>\n",
       "      <td>66.921315</td>\n",
       "      <td>19.591596</td>\n",
       "      <td>69.762117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>11.540171</td>\n",
       "      <td>98.091743</td>\n",
       "      <td>2.539126</td>\n",
       "      <td>96.302337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.500225</td>\n",
       "      <td>98.091228</td>\n",
       "      <td>2.525335</td>\n",
       "      <td>96.387583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>11.408549</td>\n",
       "      <td>98.032325</td>\n",
       "      <td>2.521022</td>\n",
       "      <td>96.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.523653</td>\n",
       "      <td>98.059881</td>\n",
       "      <td>2.514548</td>\n",
       "      <td>96.481879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>11.850556</td>\n",
       "      <td>97.938305</td>\n",
       "      <td>2.567809</td>\n",
       "      <td>96.286315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Train Loss  Train Accuracy  Test Loss  Test Accuracy\n",
       "0    398.512022       22.477906  28.562596      49.349745\n",
       "1    307.588292       41.568761  23.551324      59.000088\n",
       "2    257.268621       52.381293  23.691961      61.859415\n",
       "3    216.044611       60.483571  19.275652      68.197768\n",
       "4    185.657849       66.921315  19.591596      69.762117\n",
       "..          ...             ...        ...            ...\n",
       "205   11.540171       98.091743   2.539126      96.302337\n",
       "206   11.500225       98.091228   2.525335      96.387583\n",
       "207   11.408549       98.032325   2.521022      96.446644\n",
       "208   11.523653       98.059881   2.514548      96.481879\n",
       "209   11.850556       97.938305   2.567809      96.286315\n",
       "\n",
       "[210 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_eval = pd.read_csv('train_test_eval.csv')\n",
    "train_test_eval[['Train Loss', 'Train Accuracy', 'Test Loss', 'Test Accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        208.000000\n",
       "Train Loss         11.523653\n",
       "Train Accuracy     98.059881\n",
       "Test Loss           2.514548\n",
       "Test Accuracy      96.481879\n",
       "Name: 208, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_eval.iloc[208]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
