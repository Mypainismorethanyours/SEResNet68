{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_optimizer import Ranger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torchsummary import summary\n",
    "from SE_ResNet_55 import ResNet55, BasicBlock\n",
    "from SE_ResNet_68 import ResNet68, BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/Nikunj-Gupta/Efficient_ResNets/blob/master/lookahead.py\n",
    "\n",
    "'''\n",
    "PyTorch implement of 'Lookahead Optimizer: k steps forward, 1 step back', arXiv:1907.08610\n",
    "'''\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters:  4697742\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# # Define a 55 layers ResNet with 4 residual layers\n",
    "# net = ResNet55(\n",
    "#     block=BasicBlock, \n",
    "#     num_blocks=[2, 2, 2, 2],               # N: number of Residual Layers | Bi:Residual blocks in Residual Layer i \n",
    "#     conv_kernel_sizes=[3, 3, 3, 3],        # Fi: Conv. kernel size in Residual Layer i \n",
    "#     shortcut_kernel_sizes=[1, 1, 1, 1] ,   # Ki: Skip connection kernel size in Residual Layer i \n",
    "#     num_channels=[64, 128, 232, 268],      # Ci: # channels in Residual Layer i \n",
    "#     avg_pool_kernel_size=8,                # P: Average pool kernel size \n",
    "#     drop=0,                                # use dropout with drop proportion \n",
    "#     squeeze_and_excitation=1               # Enable/disable Squeeze-and-Excitation Block \n",
    "#     ) \n",
    "\n",
    "# Define a 68 layers ResNet with 3 residual layers\n",
    "net = ResNet68(\n",
    "    block=BasicBlock, \n",
    "    num_blocks=[4, 4, 3],                    # N: number of Residual Layers | Bi:Residual blocks in Residual Layer i \n",
    "    conv_kernel_sizes=[3, 3, 3],             # Fi: Conv. kernel size in Residual Layer i \n",
    "    shortcut_kernel_sizes=[1, 1, 1] ,        # Ki: Skip connection kernel size in Residual Layer i \n",
    "    num_channels=64,                         # Ci: # channels in Residual Layer i \n",
    "    avg_pool_kernel_size=8,                  # P: Average pool kernel size \n",
    "    drop=0,                                  # use dropout with drop proportion \n",
    "    squeeze_and_excitation=1                 # Enable/disable Squeeze-and-Excitation Block \n",
    "    ) \n",
    "\n",
    "# total_params = 0 \n",
    "# for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "#     total_params += np.prod(x.data.numpy().shape)\n",
    "# print('Total Parameters: ', total_params) \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = net.to(device)\n",
    "\n",
    "# Calculate the number of parameters for training\n",
    "summary(net, input_size=(3, 32, 32))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913996458053589, 0.48215845227241516, 0.44653093814849854), (0.2470322549343109, 0.24348513782024384, 0.26158788800239563))\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913996458053589, 0.48215845227241516, 0.44653093814849854), (0.2470322549343109, 0.24348513782024384, 0.26158788800239563))\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=transform_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers = 0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 0)\n",
    "# test_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers = 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers = 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.1, weight_decay=0.0005)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = Ranger(net.parameters(), lr=0.1, weight_decay=0.0005)\n",
    "optimizer = Lookahead(optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005), k=5, alpha=0.5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_losses = [] \n",
    "    train_acc = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(net.parameters(), clip_value=0.1) \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() \n",
    "\n",
    "        train_acc.append(100.*correct/total) \n",
    "        # print('Batch_idx: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'% (batch_idx, train_loss/(batch_idx+1), 100.*correct/total, correct, total)) \n",
    "    print('train_loss:', np.mean(train_losses)) \n",
    "    print('train_accuracy:', str(np.mean(train_acc)) + '%')\n",
    "    train_loss_per_epoch.append(np.mean(train_losses))\n",
    "    train_acc_per_epoch.append(np.mean(train_acc))\n",
    "    \n",
    "    \n",
    "# Testing \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_losses = [] \n",
    "    test_acc = [] \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() \n",
    "            test_acc.append(100.*correct/total) \n",
    "            # print('Batch_idx: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'% ( batch_idx, test_loss/(batch_idx+1), 100.*correct/total, correct, total)) \n",
    "        print('test_loss:', np.mean(test_losses)) \n",
    "        print('test_accuracy:', str(np.mean(test_acc)) + '%')\n",
    "        test_loss_per_epoch.append(np.mean(test_losses))\n",
    "        test_acc_per_epoch.append(np.mean(test_acc))\n",
    "        \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc: \n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(state, './best.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train_loss: 398.5120216037916\n",
      "train_accuracy: 22.477906239512365%\n",
      "test_loss: 28.56259649693966\n",
      "test_accuracy: 49.34974474725279%\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "train_loss: 307.58829151120636\n",
      "train_accuracy: 41.56876068997233%\n",
      "test_loss: 23.551323956251146\n",
      "test_accuracy: 59.00008830437446%\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "train_loss: 257.26862115109975\n",
      "train_accuracy: 52.3812932607036%\n",
      "test_loss: 23.69196097403765\n",
      "test_accuracy: 61.859415149682306%\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "train_loss: 216.04461117062118\n",
      "train_accuracy: 60.48357072980576%\n",
      "test_loss: 19.275652007758616\n",
      "test_accuracy: 68.19776817523606%\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "train_loss: 185.65784910892893\n",
      "train_accuracy: 66.9213146067268%\n",
      "test_loss: 19.59159562140703\n",
      "test_accuracy: 69.76211739035519%\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "train_loss: 165.3997737126582\n",
      "train_accuracy: 70.15169789837763%\n",
      "test_loss: 13.238767424225808\n",
      "test_accuracy: 77.79207754988519%\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "train_loss: 150.87508081612378\n",
      "train_accuracy: 73.36553136812228%\n",
      "test_loss: 11.869544850289822\n",
      "test_accuracy: 80.10108909025705%\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "train_loss: 141.56304711896135\n",
      "train_accuracy: 74.89507967575031%\n",
      "test_loss: 12.82439828068018\n",
      "test_accuracy: 78.97048575676868%\n",
      "\n",
      "Epoch: 8\n",
      "train_loss: 135.26473526210737\n",
      "train_accuracy: 75.9526908370981%\n",
      "test_loss: 11.749099210649728\n",
      "test_accuracy: 80.31013783567205%\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "train_loss: 125.01736866574153\n",
      "train_accuracy: 77.92842109547571%\n",
      "test_loss: 14.862713167071343\n",
      "test_accuracy: 75.86008867512031%\n",
      "\n",
      "Epoch: 10\n",
      "train_loss: 121.48488912519896\n",
      "train_accuracy: 78.51866522428038%\n",
      "test_loss: 8.840851321071387\n",
      "test_accuracy: 85.49059561926849%\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "train_loss: 116.12502665028853\n",
      "train_accuracy: 79.5834227367976%\n",
      "test_loss: 10.329842928797007\n",
      "test_accuracy: 82.65915360018178%\n",
      "\n",
      "Epoch: 12\n",
      "train_loss: 113.94315724650308\n",
      "train_accuracy: 80.08255600023625%\n",
      "test_loss: 12.115839321166277\n",
      "test_accuracy: 80.69274460852596%\n",
      "\n",
      "Epoch: 13\n",
      "train_loss: 109.2270312305454\n",
      "train_accuracy: 80.60947828181467%\n",
      "test_loss: 14.263151261210442\n",
      "test_accuracy: 77.76590158663866%\n",
      "\n",
      "Epoch: 14\n",
      "train_loss: 106.55764245415283\n",
      "train_accuracy: 81.19554855669492%\n",
      "test_loss: 11.300986211001874\n",
      "test_accuracy: 80.90739243619734%\n",
      "\n",
      "Epoch: 15\n",
      "train_loss: 103.56310844954932\n",
      "train_accuracy: 81.49320715657373%\n",
      "test_loss: 8.110743064433336\n",
      "test_accuracy: 86.90373571389905%\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "train_loss: 102.12454763054848\n",
      "train_accuracy: 82.08789505770848%\n",
      "test_loss: 10.466574357450009\n",
      "test_accuracy: 82.9865038737659%\n",
      "\n",
      "Epoch: 17\n",
      "train_loss: 100.03184251308136\n",
      "train_accuracy: 82.47142278680131%\n",
      "test_loss: 9.910887871682643\n",
      "test_accuracy: 83.50754976080076%\n",
      "\n",
      "Epoch: 18\n",
      "train_loss: 99.1593959421453\n",
      "train_accuracy: 82.41513843394144%\n",
      "test_loss: 9.508500860631466\n",
      "test_accuracy: 85.16494762035127%\n",
      "\n",
      "Epoch: 19\n",
      "train_loss: 95.59374089504752\n",
      "train_accuracy: 83.56092261150128%\n",
      "test_loss: 13.03718347325921\n",
      "test_accuracy: 79.8516022543441%\n",
      "\n",
      "Epoch: 20\n",
      "train_loss: 94.0955282437527\n",
      "train_accuracy: 83.39044073804577%\n",
      "test_loss: 7.155796720832586\n",
      "test_accuracy: 88.15098016452161%\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "train_loss: 92.26666903617742\n",
      "train_accuracy: 83.83320163058008%\n",
      "test_loss: 7.450006913393736\n",
      "test_accuracy: 87.63196713428027%\n",
      "\n",
      "Epoch: 22\n",
      "train_loss: 91.82985158298936\n",
      "train_accuracy: 83.90799484599314%\n",
      "test_loss: 9.225039765238762\n",
      "test_accuracy: 85.53563410757576%\n",
      "\n",
      "Epoch: 23\n",
      "train_loss: 89.64292919361378\n",
      "train_accuracy: 84.25582111094025%\n",
      "test_loss: 9.02720428928733\n",
      "test_accuracy: 85.88797860106766%\n",
      "\n",
      "Epoch: 24\n",
      "train_loss: 89.03308461549337\n",
      "train_accuracy: 84.33635804941234%\n",
      "test_loss: 15.641089165210724\n",
      "test_accuracy: 77.00758924230853%\n",
      "\n",
      "Epoch: 25\n",
      "train_loss: 88.98136068274603\n",
      "train_accuracy: 84.23318819107365%\n",
      "test_loss: 7.081797554716468\n",
      "test_accuracy: 88.3855298398025%\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "train_loss: 87.35750619331588\n",
      "train_accuracy: 84.63773867477619%\n",
      "test_loss: 7.4461805380880834\n",
      "test_accuracy: 87.87480533841797%\n",
      "\n",
      "Epoch: 27\n",
      "train_loss: 86.16771246161302\n",
      "train_accuracy: 84.93719097198101%\n",
      "test_loss: 8.08748431429267\n",
      "test_accuracy: 86.83730311960808%\n",
      "\n",
      "Epoch: 28\n",
      "train_loss: 85.95777269710055\n",
      "train_accuracy: 84.5652025303834%\n",
      "test_loss: 10.99820162281394\n",
      "test_accuracy: 82.32050896720463%\n",
      "\n",
      "Epoch: 29\n",
      "train_loss: 84.79557326382688\n",
      "train_accuracy: 85.13022327143472%\n",
      "test_loss: 7.767227613180876\n",
      "test_accuracy: 87.42646594931348%\n",
      "\n",
      "Epoch: 30\n",
      "train_loss: 84.12509762692025\n",
      "train_accuracy: 85.10234701477688%\n",
      "test_loss: 6.638911524787545\n",
      "test_accuracy: 88.939536319537%\n",
      "Saving..\n",
      "\n",
      "Epoch: 31\n",
      "train_loss: 83.67671276426987\n",
      "train_accuracy: 85.64011638612239%\n",
      "test_loss: 9.51326494589448\n",
      "test_accuracy: 84.59506301624745%\n",
      "\n",
      "Epoch: 32\n",
      "train_loss: 83.22028409558185\n",
      "train_accuracy: 85.30321376186272%\n",
      "test_loss: 7.796462482213974\n",
      "test_accuracy: 87.26046168547349%\n",
      "\n",
      "Epoch: 33\n",
      "train_loss: 82.56310253885701\n",
      "train_accuracy: 85.7126495218926%\n",
      "test_loss: 9.313997247815132\n",
      "test_accuracy: 84.86329286439452%\n",
      "\n",
      "Epoch: 34\n",
      "train_loss: 81.86534127413921\n",
      "train_accuracy: 85.48883791594723%\n",
      "test_loss: 9.14539493918419\n",
      "test_accuracy: 84.67561704220368%\n",
      "\n",
      "Epoch: 35\n",
      "train_loss: 80.37589889993448\n",
      "train_accuracy: 86.19699555697844%\n",
      "test_loss: 5.929372611641884\n",
      "test_accuracy: 90.01867747848837%\n",
      "Saving..\n",
      "\n",
      "Epoch: 36\n",
      "train_loss: 79.71098224067931\n",
      "train_accuracy: 86.24102376159755%\n",
      "test_loss: 6.2301365690305825\n",
      "test_accuracy: 89.70307652751278%\n",
      "\n",
      "Epoch: 37\n",
      "train_loss: 78.34085746212384\n",
      "train_accuracy: 86.44375115422909%\n",
      "test_loss: 8.864457020163536\n",
      "test_accuracy: 86.34744815820275%\n",
      "\n",
      "Epoch: 38\n",
      "train_loss: 77.99378304610319\n",
      "train_accuracy: 86.36706314680342%\n",
      "test_loss: 9.50104426369071\n",
      "test_accuracy: 84.96067912570352%\n",
      "\n",
      "Epoch: 39\n",
      "train_loss: 77.81048627506436\n",
      "train_accuracy: 86.69117297311422%\n",
      "test_loss: 9.560644910484552\n",
      "test_accuracy: 84.67548103836448%\n",
      "\n",
      "Epoch: 40\n",
      "train_loss: 77.60153869640492\n",
      "train_accuracy: 86.50439607717863%\n",
      "test_loss: 5.681551395729184\n",
      "test_accuracy: 90.92351042102439%\n",
      "Saving..\n",
      "\n",
      "Epoch: 41\n",
      "train_loss: 77.87257918585901\n",
      "train_accuracy: 86.3686353199286%\n",
      "test_loss: 5.562270036712289\n",
      "test_accuracy: 90.84197606032478%\n",
      "\n",
      "Epoch: 42\n",
      "train_loss: 77.81446223852733\n",
      "train_accuracy: 86.21124394604998%\n",
      "test_loss: 7.576045615598559\n",
      "test_accuracy: 87.75512401571844%\n",
      "\n",
      "Epoch: 43\n",
      "train_loss: 76.20304267565766\n",
      "train_accuracy: 86.8555281198711%\n",
      "test_loss: 6.909835837036371\n",
      "test_accuracy: 88.44412628678072%\n",
      "\n",
      "Epoch: 44\n",
      "train_loss: 75.51571590177086\n",
      "train_accuracy: 86.99516449162617%\n",
      "test_loss: 11.420867233723403\n",
      "test_accuracy: 82.45838691377254%\n",
      "\n",
      "Epoch: 45\n",
      "train_loss: 73.71191877606884\n",
      "train_accuracy: 87.31141985840186%\n",
      "test_loss: 5.815244195610285\n",
      "test_accuracy: 90.51214709818305%\n",
      "\n",
      "Epoch: 46\n",
      "train_loss: 75.48729461827851\n",
      "train_accuracy: 86.47339043216304%\n",
      "test_loss: 7.20216554030776\n",
      "test_accuracy: 88.42362250948563%\n",
      "\n",
      "Epoch: 47\n",
      "train_loss: 73.65761667642447\n",
      "train_accuracy: 87.05292145469937%\n",
      "test_loss: 6.596727951988578\n",
      "test_accuracy: 89.15549845434192%\n",
      "\n",
      "Epoch: 48\n",
      "train_loss: 74.71439314902285\n",
      "train_accuracy: 86.75782931110149%\n",
      "test_loss: 9.622690009325742\n",
      "test_accuracy: 85.8106134418105%\n",
      "\n",
      "Epoch: 49\n",
      "train_loss: 71.87815667422073\n",
      "train_accuracy: 87.3299422987119%\n",
      "test_loss: 9.22164303213358\n",
      "test_accuracy: 86.25421884330181%\n",
      "\n",
      "Epoch: 50\n",
      "train_loss: 73.1372407713281\n",
      "train_accuracy: 87.01175191673413%\n",
      "test_loss: 5.544766456261277\n",
      "test_accuracy: 90.35424574795279%\n",
      "\n",
      "Epoch: 51\n",
      "train_loss: 72.23728202306249\n",
      "train_accuracy: 87.31660958630464%\n",
      "test_loss: 5.7754639249294994\n",
      "test_accuracy: 90.22352292811271%\n",
      "\n",
      "Epoch: 52\n",
      "train_loss: 72.10136831145914\n",
      "train_accuracy: 87.41594386540237%\n",
      "test_loss: 9.05227120667696\n",
      "test_accuracy: 86.73176734340782%\n",
      "\n",
      "Epoch: 53\n",
      "train_loss: 72.27702638072431\n",
      "train_accuracy: 87.33685008630066%\n",
      "test_loss: 6.959715526923537\n",
      "test_accuracy: 88.92151381942537%\n",
      "\n",
      "Epoch: 54\n",
      "train_loss: 70.88967296885103\n",
      "train_accuracy: 87.72894448361019%\n",
      "test_loss: 8.903381114825606\n",
      "test_accuracy: 85.56940315504468%\n",
      "\n",
      "Epoch: 55\n",
      "train_loss: 69.2373177027687\n",
      "train_accuracy: 88.10839138692461%\n",
      "test_loss: 5.3320052023977045\n",
      "test_accuracy: 91.27955317793851%\n",
      "Saving..\n",
      "\n",
      "Epoch: 56\n",
      "train_loss: 70.09560220564722\n",
      "train_accuracy: 87.58991175268358%\n",
      "test_loss: 6.307924540340901\n",
      "test_accuracy: 90.60020550586891%\n",
      "\n",
      "Epoch: 57\n",
      "train_loss: 69.8058446016153\n",
      "train_accuracy: 87.82264425114425%\n",
      "test_loss: 7.367960000038147\n",
      "test_accuracy: 87.96901029827703%\n",
      "\n",
      "Epoch: 58\n",
      "train_loss: 69.2929907829103\n",
      "train_accuracy: 87.9770918550321%\n",
      "test_loss: 6.491409482434392\n",
      "test_accuracy: 89.4451122577225%\n",
      "\n",
      "Epoch: 59\n",
      "train_loss: 68.51932217714274\n",
      "train_accuracy: 87.83665842232816%\n",
      "test_loss: 8.234152866527438\n",
      "test_accuracy: 87.35204030718918%\n",
      "\n",
      "Epoch: 60\n",
      "train_loss: 68.97008691861501\n",
      "train_accuracy: 88.04497322377145%\n",
      "test_loss: 5.35573537684977\n",
      "test_accuracy: 91.1859398200875%\n",
      "Saving..\n",
      "\n",
      "Epoch: 61\n",
      "train_loss: 67.2538876190896\n",
      "train_accuracy: 88.12434502321936%\n",
      "test_loss: 5.887897461280227\n",
      "test_accuracy: 90.05953129949675%\n",
      "\n",
      "Epoch: 62\n",
      "train_loss: 66.92694563763526\n",
      "train_accuracy: 88.33067838789121%\n",
      "test_loss: 6.161355244368314\n",
      "test_accuracy: 89.53510379579463%\n",
      "\n",
      "Epoch: 63\n",
      "train_loss: 67.43673043402717\n",
      "train_accuracy: 88.2173065014983%\n",
      "test_loss: 8.319427633285523\n",
      "test_accuracy: 86.41883507554681%\n",
      "\n",
      "Epoch: 64\n",
      "train_loss: 67.01472915068764\n",
      "train_accuracy: 88.45089511884424%\n",
      "test_loss: 7.117523811012506\n",
      "test_accuracy: 88.66595055335877%\n",
      "\n",
      "Epoch: 65\n",
      "train_loss: 66.92572312781115\n",
      "train_accuracy: 88.14553258789005%\n",
      "test_loss: 5.315750595927239\n",
      "test_accuracy: 91.60638689160905%\n",
      "Saving..\n",
      "\n",
      "Epoch: 66\n",
      "train_loss: 66.21053528888604\n",
      "train_accuracy: 88.33780644563483%\n",
      "test_loss: 6.515133104845882\n",
      "test_accuracy: 89.86814923349895%\n",
      "\n",
      "Epoch: 67\n",
      "train_loss: 65.69133166721105\n",
      "train_accuracy: 88.54928394094328%\n",
      "test_loss: 6.303458287194371\n",
      "test_accuracy: 90.327941196548%\n",
      "\n",
      "Epoch: 68\n",
      "train_loss: 65.64974292411523\n",
      "train_accuracy: 88.50478497374671%\n",
      "test_loss: 6.047462334483862\n",
      "test_accuracy: 90.01775867988309%\n",
      "\n",
      "Epoch: 69\n",
      "train_loss: 65.65627064134763\n",
      "train_accuracy: 88.44598959755245%\n",
      "test_loss: 8.168938936293126\n",
      "test_accuracy: 87.1088647925238%\n",
      "\n",
      "Epoch: 70\n",
      "train_loss: 64.88767143096918\n",
      "train_accuracy: 88.58657220313154%\n",
      "test_loss: 5.443719404563308\n",
      "test_accuracy: 90.76201859896302%\n",
      "\n",
      "Epoch: 71\n",
      "train_loss: 64.1337252902939\n",
      "train_accuracy: 88.65893959524277%\n",
      "test_loss: 5.444219310209155\n",
      "test_accuracy: 90.9359221456406%\n",
      "\n",
      "Epoch: 72\n",
      "train_loss: 64.29798651141736\n",
      "train_accuracy: 88.68142997390363%\n",
      "test_loss: 5.814487323909998\n",
      "test_accuracy: 90.49432567218881%\n",
      "\n",
      "Epoch: 73\n",
      "train_loss: 62.55560331606804\n",
      "train_accuracy: 89.40000648738867%\n",
      "test_loss: 7.399399057775736\n",
      "test_accuracy: 88.3512251113439%\n",
      "\n",
      "Epoch: 74\n",
      "train_loss: 61.56413400702922\n",
      "train_accuracy: 89.43460658472243%\n",
      "test_loss: 9.327232185006142\n",
      "test_accuracy: 86.05361517963077%\n",
      "\n",
      "Epoch: 75\n",
      "train_loss: 61.49799483964968\n",
      "train_accuracy: 89.26796968891723%\n",
      "test_loss: 5.453273428976535\n",
      "test_accuracy: 91.14775644325033%\n",
      "\n",
      "Epoch: 76\n",
      "train_loss: 62.79514959751798\n",
      "train_accuracy: 89.06185907693683%\n",
      "test_loss: 5.71952559389174\n",
      "test_accuracy: 90.59979539606378%\n",
      "\n",
      "Epoch: 77\n",
      "train_loss: 61.99945243148853\n",
      "train_accuracy: 89.24642791990017%\n",
      "test_loss: 7.450608198344708\n",
      "test_accuracy: 88.0496454816903%\n",
      "\n",
      "Epoch: 78\n",
      "train_loss: 60.98131871753184\n",
      "train_accuracy: 89.24991340901543%\n",
      "test_loss: 9.81777736544609\n",
      "test_accuracy: 85.16121133278106%\n",
      "\n",
      "Epoch: 79\n",
      "train_loss: 59.72838327052343\n",
      "train_accuracy: 89.72598109896578%\n",
      "test_loss: 6.511450467631221\n",
      "test_accuracy: 89.44948104962205%\n",
      "\n",
      "Epoch: 80\n",
      "train_loss: 60.327856868962805\n",
      "train_accuracy: 89.1517670446276%\n",
      "test_loss: 4.882531569153071\n",
      "test_accuracy: 91.9332138787344%\n",
      "Saving..\n",
      "\n",
      "Epoch: 81\n",
      "train_loss: 59.80807684689684\n",
      "train_accuracy: 89.54009799408432%\n",
      "test_loss: 4.801792693883181\n",
      "test_accuracy: 92.34000667890275%\n",
      "Saving..\n",
      "\n",
      "Epoch: 82\n",
      "train_loss: 59.204569962552135\n",
      "train_accuracy: 89.76743413397571%\n",
      "test_loss: 5.610434227436781\n",
      "test_accuracy: 91.0607516732091%\n",
      "\n",
      "Epoch: 83\n",
      "train_loss: 58.28556360148103\n",
      "train_accuracy: 90.08035046377762%\n",
      "test_loss: 5.654602019488811\n",
      "test_accuracy: 91.04093535897607%\n",
      "\n",
      "Epoch: 84\n",
      "train_loss: 59.39372570690749\n",
      "train_accuracy: 89.92761085633533%\n",
      "test_loss: 7.41529357554391\n",
      "test_accuracy: 88.86897807514626%\n",
      "\n",
      "Epoch: 85\n",
      "train_loss: 58.19207633856465\n",
      "train_accuracy: 89.78855613881936%\n",
      "test_loss: 4.89859689027071\n",
      "test_accuracy: 92.20354455144144%\n",
      "\n",
      "Epoch: 86\n",
      "train_loss: 57.52431671847315\n",
      "train_accuracy: 90.10959251019644%\n",
      "test_loss: 4.950528017431497\n",
      "test_accuracy: 92.26200708515515%\n",
      "\n",
      "Epoch: 87\n",
      "train_loss: 57.94654662335468\n",
      "train_accuracy: 90.09729125612395%\n",
      "test_loss: 5.8446704622358086\n",
      "test_accuracy: 90.75319260552801%\n",
      "\n",
      "Epoch: 88\n",
      "train_loss: 55.937495057692615\n",
      "train_accuracy: 90.46986208920563%\n",
      "test_loss: 6.207487746700645\n",
      "test_accuracy: 90.4067243172531%\n",
      "\n",
      "Epoch: 89\n",
      "train_loss: 56.04854643653573\n",
      "train_accuracy: 90.07083820990128%\n",
      "test_loss: 5.928776534646749\n",
      "test_accuracy: 90.71578440763741%\n",
      "\n",
      "Epoch: 90\n",
      "train_loss: 55.33626322272946\n",
      "train_accuracy: 90.44158282958766%\n",
      "test_loss: 4.719603335857391\n",
      "test_accuracy: 92.23834220569867%\n",
      "Saving..\n",
      "\n",
      "Epoch: 91\n",
      "train_loss: 55.2496053827998\n",
      "train_accuracy: 90.24149993003651%\n",
      "test_loss: 5.834301806241274\n",
      "test_accuracy: 90.66209699656267%\n",
      "\n",
      "Epoch: 92\n",
      "train_loss: 54.65192419292448\n",
      "train_accuracy: 90.57984305646052%\n",
      "test_loss: 5.68150135204196\n",
      "test_accuracy: 91.22776523848488%\n",
      "\n",
      "Epoch: 93\n",
      "train_loss: 54.72512809493963\n",
      "train_accuracy: 90.44522989675761%\n",
      "test_loss: 5.777464063093066\n",
      "test_accuracy: 91.24422616710237%\n",
      "\n",
      "Epoch: 94\n",
      "train_loss: 52.79245008493933\n",
      "train_accuracy: 90.71192723808184%\n",
      "test_loss: 6.667330345511436\n",
      "test_accuracy: 89.61660727869366%\n",
      "\n",
      "Epoch: 95\n",
      "train_loss: 55.12626379116646\n",
      "train_accuracy: 90.30071433713867%\n",
      "test_loss: 4.6320893004536625\n",
      "test_accuracy: 92.33413361322977%\n",
      "Saving..\n",
      "\n",
      "Epoch: 96\n",
      "train_loss: 53.90957546026429\n",
      "train_accuracy: 90.53791339779465%\n",
      "test_loss: 4.934445177763701\n",
      "test_accuracy: 92.267769697591%\n",
      "\n",
      "Epoch: 97\n",
      "train_loss: 53.55371780461058\n",
      "train_accuracy: 90.63975406586397%\n",
      "test_loss: 6.803760772198439\n",
      "test_accuracy: 89.870777087273%\n",
      "\n",
      "Epoch: 98\n",
      "train_loss: 53.065968553428455\n",
      "train_accuracy: 90.67613010993027%\n",
      "test_loss: 6.275424585863948\n",
      "test_accuracy: 89.52065138727806%\n",
      "\n",
      "Epoch: 99\n",
      "train_loss: 51.94471315311654\n",
      "train_accuracy: 91.07265641853141%\n",
      "test_loss: 7.437609111517668\n",
      "test_accuracy: 88.93077269112436%\n",
      "\n",
      "Epoch: 100\n",
      "train_loss: 50.45435645943865\n",
      "train_accuracy: 91.06629363888803%\n",
      "test_loss: 4.445081152766943\n",
      "test_accuracy: 92.4407913849387%\n",
      "Saving..\n",
      "\n",
      "Epoch: 101\n",
      "train_loss: 50.334518560553754\n",
      "train_accuracy: 91.1899365198438%\n",
      "test_loss: 4.790747775137424\n",
      "test_accuracy: 92.47165921805%\n",
      "\n",
      "Epoch: 102\n",
      "train_loss: 50.58502208691119\n",
      "train_accuracy: 91.38886864141134%\n",
      "test_loss: 5.3943371590226885\n",
      "test_accuracy: 91.33640112323215%\n",
      "\n",
      "Epoch: 103\n",
      "train_loss: 49.72478514970721\n",
      "train_accuracy: 91.36620710427594%\n",
      "test_loss: 5.20222186781466\n",
      "test_accuracy: 92.40048105846151%\n",
      "\n",
      "Epoch: 104\n",
      "train_loss: 49.325955990825776\n",
      "train_accuracy: 91.24752274497179%\n",
      "test_loss: 5.088807224854827\n",
      "test_accuracy: 91.54609429676569%\n",
      "\n",
      "Epoch: 105\n",
      "train_loss: 49.56471201316322\n",
      "train_accuracy: 91.38009667015393%\n",
      "test_loss: 4.637947217002511\n",
      "test_accuracy: 92.54882024331623%\n",
      "Saving..\n",
      "\n",
      "Epoch: 106\n",
      "train_loss: 48.88124049018563\n",
      "train_accuracy: 91.50734115624194%\n",
      "test_loss: 4.539990048855543\n",
      "test_accuracy: 93.02007754479496%\n",
      "Saving..\n",
      "\n",
      "Epoch: 107\n",
      "train_loss: 47.475932900901036\n",
      "train_accuracy: 91.69660058965714%\n",
      "test_loss: 5.3867970366030935\n",
      "test_accuracy: 91.7287222704983%\n",
      "\n",
      "Epoch: 108\n",
      "train_loss: 48.121199708841644\n",
      "train_accuracy: 91.3761206377602%\n",
      "test_loss: 4.957128394022584\n",
      "test_accuracy: 92.19702477327776%\n",
      "\n",
      "Epoch: 109\n",
      "train_loss: 47.303124222227986\n",
      "train_accuracy: 91.89422924355742%\n",
      "test_loss: 7.810837483778596\n",
      "test_accuracy: 88.89036070277082%\n",
      "\n",
      "Epoch: 110\n",
      "train_loss: 46.102939844302966\n",
      "train_accuracy: 92.0299553525571%\n",
      "test_loss: 4.456182098016143\n",
      "test_accuracy: 92.88071778616411%\n",
      "\n",
      "Epoch: 111\n",
      "train_loss: 46.872656626660195\n",
      "train_accuracy: 91.97965722168%\n",
      "test_loss: 4.714265663176775\n",
      "test_accuracy: 93.13983951697226%\n",
      "\n",
      "Epoch: 112\n",
      "train_loss: 45.858612689482584\n",
      "train_accuracy: 91.9268550835085%\n",
      "test_loss: 4.756800250336528\n",
      "test_accuracy: 92.31672794528187%\n",
      "\n",
      "Epoch: 113\n",
      "train_loss: 44.86006511740215\n",
      "train_accuracy: 92.27846525779628%\n",
      "test_loss: 4.844113847613334\n",
      "test_accuracy: 92.7651757101278%\n",
      "\n",
      "Epoch: 114\n",
      "train_loss: 44.75391006818437\n",
      "train_accuracy: 92.15739276997733%\n",
      "test_loss: 4.664315938204527\n",
      "test_accuracy: 92.99232774469563%\n",
      "\n",
      "Epoch: 115\n",
      "train_loss: 44.11653649120989\n",
      "train_accuracy: 92.23501644114745%\n",
      "test_loss: 4.0595047630369665\n",
      "test_accuracy: 93.37728315679831%\n",
      "Saving..\n",
      "\n",
      "Epoch: 116\n",
      "train_loss: 42.30398310900039\n",
      "train_accuracy: 92.67916083742283%\n",
      "test_loss: 4.246927292644978\n",
      "test_accuracy: 93.21822401093173%\n",
      "\n",
      "Epoch: 117\n",
      "train_loss: 43.043999174290605\n",
      "train_accuracy: 92.6133958756232%\n",
      "test_loss: 4.749554412066937\n",
      "test_accuracy: 92.29059130737092%\n",
      "\n",
      "Epoch: 118\n",
      "train_loss: 42.88779198959508\n",
      "train_accuracy: 92.37271238060713%\n",
      "test_loss: 5.7218553822487594\n",
      "test_accuracy: 91.0090817150033%\n",
      "\n",
      "Epoch: 119\n",
      "train_loss: 40.783420475947736\n",
      "train_accuracy: 93.05656551098058%\n",
      "test_loss: 5.008225696906448\n",
      "test_accuracy: 92.26780601492449%\n",
      "\n",
      "Epoch: 120\n",
      "train_loss: 41.52972507210034\n",
      "train_accuracy: 92.93972067512378%\n",
      "test_loss: 3.862403558380902\n",
      "test_accuracy: 94.01641041754772%\n",
      "Saving..\n",
      "\n",
      "Epoch: 121\n",
      "train_loss: 42.302316915958436\n",
      "train_accuracy: 92.747818509132%\n",
      "test_loss: 4.051818858459592\n",
      "test_accuracy: 93.77902432674966%\n",
      "\n",
      "Epoch: 122\n",
      "train_loss: 40.289601449028154\n",
      "train_accuracy: 92.96493763885022%\n",
      "test_loss: 5.225495221838355\n",
      "test_accuracy: 91.9729852157869%\n",
      "\n",
      "Epoch: 123\n",
      "train_loss: 40.94959391941271\n",
      "train_accuracy: 92.88554769244844%\n",
      "test_loss: 4.810104943439365\n",
      "test_accuracy: 92.80130478382765%\n",
      "\n",
      "Epoch: 124\n",
      "train_loss: 39.26155614711897\n",
      "train_accuracy: 93.31640973861225%\n",
      "test_loss: 6.481406252458692\n",
      "test_accuracy: 90.37593432855367%\n",
      "\n",
      "Epoch: 125\n",
      "train_loss: 39.82061778517711\n",
      "train_accuracy: 92.96755502204952%\n",
      "test_loss: 4.052829557657242\n",
      "test_accuracy: 93.65822042019313%\n",
      "\n",
      "Epoch: 126\n",
      "train_loss: 38.551477600794165\n",
      "train_accuracy: 93.25862483513787%\n",
      "test_loss: 4.233464972674847\n",
      "test_accuracy: 93.23369413609468%\n",
      "\n",
      "Epoch: 127\n",
      "train_loss: 38.138358163349615\n",
      "train_accuracy: 93.5174095181839%\n",
      "test_loss: 3.88656633682549\n",
      "test_accuracy: 93.6666028939114%\n",
      "\n",
      "Epoch: 128\n",
      "train_loss: 37.8245650422009\n",
      "train_accuracy: 93.37516603141383%\n",
      "test_loss: 5.12716023735702\n",
      "test_accuracy: 92.21910835828551%\n",
      "\n",
      "Epoch: 129\n",
      "train_loss: 37.63458349515715\n",
      "train_accuracy: 93.58597103836476%\n",
      "test_loss: 4.815935921296477\n",
      "test_accuracy: 92.83783950663553%\n",
      "\n",
      "Epoch: 130\n",
      "train_loss: 36.525277591517664\n",
      "train_accuracy: 93.54796706977216%\n",
      "test_loss: 3.886155324801803\n",
      "test_accuracy: 93.94737091988286%\n",
      "\n",
      "Epoch: 131\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "train_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "train_acc_per_epoch = []\n",
    "test_acc_per_epoch = []\n",
    "for epoch in range(210): \n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1619.177015</td>\n",
       "      <td>20.226219</td>\n",
       "      <td>132.875744</td>\n",
       "      <td>40.173329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243.904046</td>\n",
       "      <td>41.361891</td>\n",
       "      <td>87.739549</td>\n",
       "      <td>59.356718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1008.477470</td>\n",
       "      <td>53.931819</td>\n",
       "      <td>112.362868</td>\n",
       "      <td>55.608871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>863.560530</td>\n",
       "      <td>60.753004</td>\n",
       "      <td>64.298454</td>\n",
       "      <td>71.703157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>780.544254</td>\n",
       "      <td>64.604683</td>\n",
       "      <td>140.398815</td>\n",
       "      <td>53.969890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>64.926888</td>\n",
       "      <td>97.199416</td>\n",
       "      <td>9.828154</td>\n",
       "      <td>95.998920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>63.866885</td>\n",
       "      <td>97.318172</td>\n",
       "      <td>9.817834</td>\n",
       "      <td>95.925415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>66.104326</td>\n",
       "      <td>97.154664</td>\n",
       "      <td>9.927568</td>\n",
       "      <td>95.725125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>65.536809</td>\n",
       "      <td>97.259325</td>\n",
       "      <td>9.769878</td>\n",
       "      <td>95.851526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>65.436616</td>\n",
       "      <td>97.396771</td>\n",
       "      <td>9.687873</td>\n",
       "      <td>96.071812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train Loss  Train Accuracy   Test Loss  Test Accuracy\n",
       "0    1619.177015       20.226219  132.875744      40.173329\n",
       "1    1243.904046       41.361891   87.739549      59.356718\n",
       "2    1008.477470       53.931819  112.362868      55.608871\n",
       "3     863.560530       60.753004   64.298454      71.703157\n",
       "4     780.544254       64.604683  140.398815      53.969890\n",
       "..           ...             ...         ...            ...\n",
       "205    64.926888       97.199416    9.828154      95.998920\n",
       "206    63.866885       97.318172    9.817834      95.925415\n",
       "207    66.104326       97.154664    9.927568      95.725125\n",
       "208    65.536809       97.259325    9.769878      95.851526\n",
       "209    65.436616       97.396771    9.687873      96.071812\n",
       "\n",
       "[210 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_eval = pd.DataFrame({\n",
    "    \"Train Loss\": train_loss_per_epoch,\n",
    "    \"Train Accuracy\": train_acc_per_epoch,\n",
    "    \"Test Loss\": test_loss_per_epoch,\n",
    "    \"Test Accuracy\": test_acc_per_epoch\n",
    "})\n",
    "train_test_eval.to_csv('./train_test_eval.csv')\n",
    "train_test_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
